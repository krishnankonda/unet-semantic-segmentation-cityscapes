{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CAMAFfnpeUBv","outputId":"118ced57-bb3c-4a41-d08e-f324beae3f05","collapsed":true,"executionInfo":{"status":"ok","timestamp":1747299095252,"user_tz":300,"elapsed":7159,"user":{"displayName":"mk chung","userId":"05412849786518098631"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.11/dist-packages (0.5.0)\n","Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.31.1)\n","Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.0.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.2.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\n","Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.15)\n","Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.13.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.4.26)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"]}],"source":["# CS444 U-Net Project Setup for Semantic Segmentation\n","# Attention U-Net(MobileNetV2) on Cityscapes dataset\n","\n","# Step 1: Install Required Libraries\n","!pip install torch torchvision\n","!pip install segmentation-models-pytorch\n","!pip install matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6ZJKRlgeaD3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747299095850,"user_tz":300,"elapsed":588,"user":{"displayName":"mk chung","userId":"05412849786518098631"}},"outputId":"d5704e24-9ea5-438a-e9d1-e7dbd4a419d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Step 2: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["ls '/content/drive/MyDrive/U-Net Segmentation Project/'"],"metadata":{"id":"9dTNH29BVUCZ","executionInfo":{"status":"ok","timestamp":1747299095945,"user_tz":300,"elapsed":93,"user":{"displayName":"mk chung","userId":"05412849786518098631"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"11501515-26c3-4a36-bff3-bff43d04ce71"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" \u001b[0m\u001b[01;34mCityScapes\u001b[0m/                              'ResNet50 - U-Net_Segmentation.ipynb'\n","'Copy of U-Net_Segmentation.ipynb'         U-Net_Segmentation.ipynb\n","'Final Project Report.gdoc'                U-Net_Segmentation_UNet++.ipynb\n","'mobilenetv2 - U-Net_Segmentation.ipynb'\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZZIn2y9ecdi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747299480756,"user_tz":300,"elapsed":14,"user":{"displayName":"mk chung","userId":"05412849786518098631"}},"outputId":"dbe69d2d-8af1-4305-c0df-7e587d63fab4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset structure verified: 'leftImg8bit' and 'gtFine' found in /content/drive/MyDrive/U-Net Segmentation Project/CityScapes.\n"]}],"source":["# Step 3: Verify Dataset Structure\n","import os\n","\n","# Update this path to your specific Google Drive location for the Cityscapes dataset\n","root_path = '/content/drive/MyDrive/U-Net Segmentation Project/CityScapes'\n","\n","if not (os.path.exists(os.path.join(root_path, 'leftImg8bit')) and os.path.exists(os.path.join(root_path, 'gtFine'))):\n","    raise RuntimeError(f\"Ensure 'leftImg8bit' and 'gtFine' folders are in the root directory: {root_path}. Please verify the path.\")\n","else:\n","    print(f\"Dataset structure verified: 'leftImg8bit' and 'gtFine' found in {root_path}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4214,"status":"ok","timestamp":1747299485475,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"_pz2bfSLefC2","outputId":"e6241c43-7c54-43f0-c22a-21048acd9765"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.6.0+cu124\n","CUDA available: True\n","CUDA version: 12.4\n","GPU device: Tesla T4\n"]}],"source":["# Step 4: Verify GPU Environment\n","import torch\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"CUDA version: {torch.version.cuda}\")\n","    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n","    device = torch.device(\"cuda\")\n","else:\n","    print(\"No GPU detected. Training will run on CPU, which will be very slow. Ensure GPU runtime is enabled in Colab (Runtime > Change runtime type).\")\n","    device = torch.device(\"cpu\")\n","\n","# Set environment variables for CUDA debugging\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Synchronous CUDA errors\n","os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"    # Device-side assertions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2829,"status":"ok","timestamp":1747299488307,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"qvwy07W_ehly","outputId":"5d6fd915-cae9-469a-f0fe-e1727835ba88"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded train dataset with 2975 samples.\n","Successfully loaded val dataset with 500 samples.\n","Sample image shape: torch.Size([3, 128, 256])\n","Sample target shape: torch.Size([128, 256])\n","Sample target unique values: tensor([  0,   1,   2,   5,   7,   8,   9,  10,  11,  13,  18, 255])\n"]}],"source":["# Step 5: Load Cityscapes Dataset\n","from torchvision.datasets import Cityscapes\n","from torchvision import transforms\n","import numpy as np\n","from PIL import Image # Needed for target resize resampling\n","\n","# Define transforms (resize to manage memory)\n","# Original image size for Cityscapes is 1024x2048. Resizing significantly.\n","img_height, img_width = 128, 256 # Reduced size for faster training and lower memory\n","\n","transform = transforms.Compose([\n","    transforms.Resize((img_height, img_width)),\n","    transforms.ToTensor()\n","])\n","\n","def target_to_tensor(target):\n","    # Resize target segmentation map. PIL Image object is expected by Resize.\n","    target = target.resize((img_width, img_height), resample=Image.NEAREST) # Nearest neighbor for labels\n","    target_np = np.array(target, dtype=np.uint8)\n","\n","    # Map Cityscapes labels to 0-18 (for 19 classes), set others to ignore_index (255)\n","    label_map = {\n","        # name: id, trainId\n","        'unlabeled': (0, 255), 'ego vehicle': (1, 255), 'rectification border': (2, 255),\n","        'out of roi': (3, 255), 'static': (4, 255), 'dynamic': (5, 255),\n","        'ground': (6, 255), 'road': (7, 0), 'sidewalk': (8, 1),\n","        'parking': (9, 255), 'rail track': (10, 255), 'building': (11, 2),\n","        'wall': (12, 3), 'fence': (13, 4), 'guard rail': (14, 255),\n","        'bridge': (15, 255), 'tunnel': (16, 255), 'pole': (17, 5),\n","        'polegroup': (18, 255), 'traffic light': (19, 6), 'traffic sign': (20, 7),\n","        'vegetation': (21, 8), 'terrain': (22, 9), 'sky': (23, 10),\n","        'person': (24, 11), 'rider': (25, 12), 'car': (26, 13),\n","        'truck': (27, 14), 'bus': (28, 15), 'caravan': (29, 255),\n","        'trailer': (30, 255), 'train': (31, 16), 'motorcycle': (32, 17),\n","        'bicycle': (33, 18)\n","    }\n","    mapped_target = np.full_like(target_np, 255, dtype=np.uint8)\n","\n","    for cityscapes_id_tuple, train_id in label_map.items():\n","        original_id = train_id[0]\n","        target_train_id = train_id[1]\n","        if target_train_id != 255:\n","            mapped_target[target_np == original_id] = target_train_id\n","\n","    return torch.from_numpy(mapped_target).long()\n","\n","# Load training and validation datasets\n","try:\n","    train_dataset = Cityscapes(\n","        root=root_path,\n","        split='train',\n","        mode='fine',\n","        target_type='semantic',\n","        transform=transform,\n","        target_transform=target_to_tensor\n","    )\n","    val_dataset = Cityscapes(\n","        root=root_path,\n","        split='val',\n","        mode='fine',\n","        target_type='semantic',\n","        transform=transform,\n","        target_transform=target_to_tensor\n","    )\n","    print(f\"Successfully loaded train dataset with {len(train_dataset)} samples.\")\n","    print(f\"Successfully loaded val dataset with {len(val_dataset)} samples.\")\n","\n","except Exception as e:\n","    print(f\"Error loading dataset: {e}\")\n","    print(\"Please ensure your `root_path` is correct and the dataset is properly structured.\")\n","    raise\n","\n","# Verify dataset labels from a sample\n","if len(train_dataset) > 0:\n","    sample_image, sample_target = train_dataset[0]\n","    print(f\"Sample image shape: {sample_image.shape}\")\n","    print(f\"Sample target shape: {sample_target.shape}\")\n","    unique_labels = torch.unique(sample_target)\n","    print(f\"Sample target unique values: {unique_labels}\")\n","    if not (all( (unique_labels >= 0) & (unique_labels <= 18) | (unique_labels == 255) )):\n","        print(\"Warning: Unexpected label values found in sample target.\")\n","else:\n","    print(\"Train dataset is empty. Cannot verify sample.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1486,"status":"ok","timestamp":1747299489797,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"wcmRHoRUellz","outputId":"60a0e235-4a92-42c5-9a4a-fe60a1b3e15b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking train dataset integrity (first 5 samples)...\n","train dataset integrity verified for first 5 samples.\n","Checking val dataset integrity (first 5 samples)...\n","val dataset integrity verified for first 5 samples.\n"]}],"source":["# Step 5.1: Verify Dataset Integrity\n","def check_dataset_integrity(dataset, split_name, num_samples_to_check=5):\n","    print(f\"Checking {split_name} dataset integrity (first {num_samples_to_check} samples)...\")\n","    if len(dataset) == 0:\n","        print(f\"{split_name} dataset is empty. Skipping integrity check.\")\n","        return True\n","    for i in range(min(num_samples_to_check, len(dataset))):\n","        try:\n","            image, target = dataset[i]\n","            # Basic checks\n","            if not isinstance(image, torch.Tensor) or not isinstance(target, torch.Tensor):\n","                print(f\"Error at index {i}: Image or target is not a tensor.\")\n","                return False\n","            if image.shape != torch.Size([3, img_height, img_width]):\n","                 print(f\"Error at index {i}: Unexpected image shape {image.shape}.\")\n","                 return False\n","            if target.shape != torch.Size([img_height, img_width]):\n","                 print(f\"Error at index {i}: Unexpected target shape {target.shape}.\")\n","                 return False\n","            if not ((target >= 0) & (target <= 18) | (target == 255)).all():\n","                 print(f\"Error at index {i}: Target contains invalid labels {torch.unique(target)}.\")\n","                 return False\n","\n","        except Exception as e:\n","            print(f\"Error accessing sample {i} in {split_name} dataset: {e}\")\n","            return False\n","    print(f\"{split_name} dataset integrity verified for first {num_samples_to_check} samples.\")\n","    return True\n","\n","# Check train and validation datasets (first few samples)\n","if not check_dataset_integrity(train_dataset, \"train\"):\n","    raise RuntimeError(\"Train dataset integrity check failed.\")\n","if not check_dataset_integrity(val_dataset, \"val\"):\n","    raise RuntimeError(\"Validation dataset integrity check failed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1747299489823,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"4wQ7hLvpeogt","outputId":"1d4d5cec-1034-40c7-887e-033b42dc2883"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train loader created with batch size 2.\n","Validation loader created with batch size 2.\n"]}],"source":["# Step 6: Create Data Loaders\n","from torch.utils.data import DataLoader\n","\n","batch_size = 2\n","\n","if len(train_dataset) > 0:\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","else:\n","    train_loader = None\n","    print(\"Train dataset is empty. Train loader not created.\")\n","\n","if len(val_dataset) > 0:\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","else:\n","    val_loader = None\n","    print(\"Validation dataset is empty. Validation loader not created.\")\n","\n","if train_loader:\n","    print(f\"Train loader created with batch size {batch_size}.\")\n","if val_loader:\n","    print(f\"Validation loader created with batch size {batch_size}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2358,"status":"ok","timestamp":1747299493821,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"E07uINSEerSA","outputId":"cca54346-e9ab-4152-c509-09592240171b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Standard U-Net with mobilenet_v2 encoder defined.\n","Attention U-Net (SCSE) with mobilenet_v2 encoder defined.\n","U-Net++ with mobilenet_v2 encoder defined.\n"]}],"source":["# Step 7: Define Models\n","import segmentation_models_pytorch as smp\n","\n","num_classes = 19  # Based on the label mapping (0-18)\n","encoder_name = \"mobilenet_v2\"\n","\n","# Attention U-Net (with SCSE attention)\n","model_attention = smp.Unet(\n","    encoder_name=encoder_name,\n","    encoder_weights=\"imagenet\",\n","    in_channels=3,\n","    classes=num_classes,\n","    activation=None,\n","    decoder_attention_type=\"scse\" # Spatial and Channel Squeeze & Excitation\n",")\n","print(f\"Attention U-Net (SCSE) with {encoder_name} encoder defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1747299494649,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"UsTMZjp8eubU","outputId":"46c3b9a2-4241-4df8-9b3b-d6c3ae3260b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["CrossEntropyLoss defined with ignore_index=255.\n"]}],"source":["# Step 8: Define Loss Function\n","import torch.nn as nn\n","# CrossEntropyLoss expects raw logits from the model and long type targets.\n","# ignore_index=255 means that pixels with label 255 will not contribute to the loss.\n","criterion = nn.CrossEntropyLoss(ignore_index=255)\n","print(\"CrossEntropyLoss defined with ignore_index=255.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":143,"status":"ok","timestamp":1747299495865,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"tn_l-Gsvg6El","outputId":"da7ee029-e028-4613-a23f-47094ec38877"},"outputs":[{"output_type":"stream","name":"stdout","text":["Models will be saved in: /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models\n"]}],"source":["# Step 8.1: Setup Model Saving and Validation Loss Function\n","import os\n","\n","save_dir = os.path.join(root_path, 'saved_models')\n","os.makedirs(save_dir, exist_ok=True)\n","print(f\"Models will be saved in: {save_dir}\")\n","\n","def evaluate_validation_loss(model, loader, criterion, device, model_name):\n","    model.eval()\n","    running_val_loss = 0.0\n","    num_batches = len(loader)\n","\n","    if num_batches == 0:\n","        print(f\"Warning: Validation loader for {model_name} is empty. Cannot compute validation loss.\")\n","        return float('inf')\n","\n","    with torch.no_grad():\n","        for images, targets in loader:\n","            images, targets = images.to(device), targets.to(device)\n","\n","            # Ensure targets are valid before calculating loss\n","            if not ((targets >= 0) & (targets < num_classes) | (targets == 255)).all():\n","                print(f\"Validation: Invalid labels detected in targets for {model_name}! Unique: {torch.unique(targets)}. Skipping batch for loss calculation.\")\n","                pass\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, targets)\n","            running_val_loss += loss.item()\n","\n","    avg_val_loss = running_val_loss / num_batches if num_batches > 0 else float('inf')\n","    if device.type == \"cuda\":\n","        torch.cuda.empty_cache()\n","    return avg_val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsHCbdHPexJ-"},"outputs":[],"source":["# Step 9: Training Configuration\n","num_epochs = 15\n","learning_rate = 0.001\n","weight_decay = 1e-5\n","\n","def train_one_epoch(model, loader, optimizer, criterion, device, model_name):\n","    model.train()\n","    running_loss = 0.0\n","    num_batches = len(loader)\n","\n","    if num_batches == 0:\n","        print(f\"Warning: Training loader for {model_name} is empty. Skipping training epoch.\")\n","        return 0.0\n","\n","    for i, (images, targets) in enumerate(loader):\n","        images, targets = images.to(device), targets.to(device)\n","\n","        try:\n","            if not ((targets >= 0) & (targets < num_classes) | (targets == 255)).all():\n","                print(f\"Batch {i}: Invalid labels detected in targets for {model_name}! Unique: {torch.unique(targets)}. Skipping batch.\")\n","                problematic_targets = targets.clone()\n","                problematic_targets[(targets >= 0) & (targets < num_classes) | (targets == 255)] = -1\n","                print(f\"Problematic target values: {torch.unique(problematic_targets.masked_select(problematic_targets != -1))}\")\n","                continue\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","            if (i + 1) % (num_batches // 5 if num_batches >= 5 else 1) == 0: # Print 5 times per epoch\n","                print(f\"    {model_name} - Batch {i+1}/{num_batches}, Loss: {loss.item():.4f}\")\n","\n","        except RuntimeError as e:\n","            print(f\"RuntimeError during training {model_name} at batch {i}: {e}\")\n","            if \"CUDA out of memory\" in str(e):\n","                print(\"CUDA OOM: Try reducing batch size or image dimensions.\")\n","                if device.type == \"cuda\": torch.cuda.empty_cache()\n","            return float('inf') # Indicate critical error\n","\n","    epoch_loss = running_loss / num_batches if num_batches > 0 else 0\n","    if device.type == \"cuda\":\n","        torch.cuda.empty_cache()\n","    return epoch_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7487312,"status":"ok","timestamp":1747306997434,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"i3cRNapne1wP","outputId":"5a8b8716-3f5e-4d71-8e9b-75326412f59e","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Training Attention U-Net (SCSE, mobilenet_v2) ---\n","Initial GPU memory allocated for Attention U-Net: 26.56 MB\n","Epoch 1/15\n","    Attention U-Net - Batch 297/1488, Loss: 1.0064\n","    Attention U-Net - Batch 594/1488, Loss: 0.7380\n","    Attention U-Net - Batch 891/1488, Loss: 0.6810\n","    Attention U-Net - Batch 1188/1488, Loss: 0.4368\n","    Attention U-Net - Batch 1485/1488, Loss: 0.4129\n","  Attention U-Net - Epoch 1 Average Training Loss: 0.6590\n","  Attention U-Net - Epoch 1 Average Validation Loss: 0.5205\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_mobilenetv2.pth (Val Loss: 0.5205)\n","Epoch 2/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.3197\n","    Attention U-Net - Batch 594/1488, Loss: 0.4265\n","    Attention U-Net - Batch 891/1488, Loss: 0.2249\n","    Attention U-Net - Batch 1188/1488, Loss: 0.7110\n","    Attention U-Net - Batch 1485/1488, Loss: 0.4530\n","  Attention U-Net - Epoch 2 Average Training Loss: 0.4674\n","  Attention U-Net - Epoch 2 Average Validation Loss: 0.4546\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_mobilenetv2.pth (Val Loss: 0.4546)\n","Epoch 3/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.6764\n","    Attention U-Net - Batch 594/1488, Loss: 0.4210\n","    Attention U-Net - Batch 891/1488, Loss: 0.4932\n","    Attention U-Net - Batch 1188/1488, Loss: 0.6083\n","    Attention U-Net - Batch 1485/1488, Loss: 0.5410\n","  Attention U-Net - Epoch 3 Average Training Loss: 0.4148\n","  Attention U-Net - Epoch 3 Average Validation Loss: 0.4325\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_mobilenetv2.pth (Val Loss: 0.4325)\n","Epoch 4/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.2445\n","    Attention U-Net - Batch 594/1488, Loss: 0.7410\n","    Attention U-Net - Batch 891/1488, Loss: 0.2423\n","    Attention U-Net - Batch 1188/1488, Loss: 0.2896\n","    Attention U-Net - Batch 1485/1488, Loss: 0.2539\n","  Attention U-Net - Epoch 4 Average Training Loss: 0.3916\n","  Attention U-Net - Epoch 4 Average Validation Loss: 0.4314\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_mobilenetv2.pth (Val Loss: 0.4314)\n","Epoch 5/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.5653\n","    Attention U-Net - Batch 594/1488, Loss: 0.4153\n","    Attention U-Net - Batch 891/1488, Loss: 0.5278\n","    Attention U-Net - Batch 1188/1488, Loss: 0.4077\n","    Attention U-Net - Batch 1485/1488, Loss: 0.2444\n","  Attention U-Net - Epoch 5 Average Training Loss: 0.3718\n","  Attention U-Net - Epoch 5 Average Validation Loss: 0.3999\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_mobilenetv2.pth (Val Loss: 0.3999)\n","Epoch 6/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.3363\n","    Attention U-Net - Batch 594/1488, Loss: 0.5362\n","    Attention U-Net - Batch 891/1488, Loss: 0.3652\n","    Attention U-Net - Batch 1188/1488, Loss: 0.6445\n","    Attention U-Net - Batch 1485/1488, Loss: 0.2110\n","  Attention U-Net - Epoch 6 Average Training Loss: 0.3515\n","  Attention U-Net - Epoch 6 Average Validation Loss: 0.3965\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_mobilenetv2.pth (Val Loss: 0.3965)\n","Epoch 7/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.2811\n","    Attention U-Net - Batch 594/1488, Loss: 0.3054\n","    Attention U-Net - Batch 891/1488, Loss: 0.5789\n","    Attention U-Net - Batch 1188/1488, Loss: 0.4183\n","    Attention U-Net - Batch 1485/1488, Loss: 0.4427\n","  Attention U-Net - Epoch 7 Average Training Loss: 0.3394\n","  Attention U-Net - Epoch 7 Average Validation Loss: 0.3744\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_mobilenetv2.pth (Val Loss: 0.3744)\n","Epoch 8/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.2603\n","    Attention U-Net - Batch 594/1488, Loss: 0.3010\n","    Attention U-Net - Batch 891/1488, Loss: 0.2276\n","    Attention U-Net - Batch 1188/1488, Loss: 0.3295\n","    Attention U-Net - Batch 1485/1488, Loss: 0.3076\n","  Attention U-Net - Epoch 8 Average Training Loss: 0.3316\n","  Attention U-Net - Epoch 8 Average Validation Loss: 0.3934\n","Epoch 9/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.2650\n","    Attention U-Net - Batch 594/1488, Loss: 0.2245\n","    Attention U-Net - Batch 891/1488, Loss: 0.4228\n","    Attention U-Net - Batch 1188/1488, Loss: 0.2264\n","    Attention U-Net - Batch 1485/1488, Loss: 0.2216\n","  Attention U-Net - Epoch 9 Average Training Loss: 0.3227\n","  Attention U-Net - Epoch 9 Average Validation Loss: 0.3912\n","Epoch 10/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.1636\n","    Attention U-Net - Batch 594/1488, Loss: 0.2327\n","    Attention U-Net - Batch 891/1488, Loss: 0.2808\n","    Attention U-Net - Batch 1188/1488, Loss: 0.3262\n","    Attention U-Net - Batch 1485/1488, Loss: 0.4735\n","  Attention U-Net - Epoch 10 Average Training Loss: 0.3141\n","  Attention U-Net - Epoch 10 Average Validation Loss: 0.3640\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_mobilenetv2.pth (Val Loss: 0.3640)\n","Epoch 11/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.4737\n","    Attention U-Net - Batch 594/1488, Loss: 0.2420\n","    Attention U-Net - Batch 891/1488, Loss: 0.2133\n","    Attention U-Net - Batch 1188/1488, Loss: 0.2987\n","    Attention U-Net - Batch 1485/1488, Loss: 0.3856\n","  Attention U-Net - Epoch 11 Average Training Loss: 0.3063\n","  Attention U-Net - Epoch 11 Average Validation Loss: 0.3933\n","Epoch 12/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.3312\n","    Attention U-Net - Batch 594/1488, Loss: 0.2665\n","    Attention U-Net - Batch 891/1488, Loss: 0.3068\n","    Attention U-Net - Batch 1188/1488, Loss: 0.4626\n","    Attention U-Net - Batch 1485/1488, Loss: 0.3181\n","  Attention U-Net - Epoch 12 Average Training Loss: 0.3015\n","  Attention U-Net - Epoch 12 Average Validation Loss: 0.4243\n","Epoch 13/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.3780\n","    Attention U-Net - Batch 594/1488, Loss: 0.3444\n","    Attention U-Net - Batch 891/1488, Loss: 0.3264\n","    Attention U-Net - Batch 1188/1488, Loss: 0.3030\n","    Attention U-Net - Batch 1485/1488, Loss: 0.2673\n","  Attention U-Net - Epoch 13 Average Training Loss: 0.2938\n","  Attention U-Net - Epoch 13 Average Validation Loss: 0.3813\n","Epoch 14/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.3259\n","    Attention U-Net - Batch 594/1488, Loss: 0.3933\n","    Attention U-Net - Batch 891/1488, Loss: 0.2715\n","    Attention U-Net - Batch 1188/1488, Loss: 0.2366\n","    Attention U-Net - Batch 1485/1488, Loss: 0.2464\n","  Attention U-Net - Epoch 14 Average Training Loss: 0.2923\n","  Attention U-Net - Epoch 14 Average Validation Loss: 0.3787\n","Epoch 15/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.2933\n","    Attention U-Net - Batch 594/1488, Loss: 0.2274\n","    Attention U-Net - Batch 891/1488, Loss: 0.1858\n","    Attention U-Net - Batch 1188/1488, Loss: 0.3689\n","    Attention U-Net - Batch 1485/1488, Loss: 0.3065\n","  Attention U-Net - Epoch 15 Average Training Loss: 0.2876\n","  Attention U-Net - Epoch 15 Average Validation Loss: 0.3692\n","--- Attention U-Net Training Finished. Final model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_final_epoch_mobilenetv2.pth ---\n","Best validation model saved at /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_mobilenetv2.pth with val_loss: 0.3640\n"]}],"source":["# --- Training Cell 9.2: Attention U-Net ---\n","print(f\"\\n--- Training Attention U-Net (SCSE, {encoder_name}) ---\")\n","model_name_attention = \"attention_unet_scse\"\n","best_model_attention_path = os.path.join(save_dir, f\"{model_name_attention}_best_val_mobilenetv2.pth\")\n","final_model_attention_path = os.path.join(save_dir, f\"{model_name_attention}_final_epoch_mobilenetv2.pth\")\n","best_val_loss_attention = float('inf')\n","\n","if train_loader:\n","    model_attention.to(device)\n","    # Initialize optimizer with weight decay\n","    optimizer_attention = torch.optim.Adam(model_attention.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","    # Initialize learning rate scheduler\n","    scheduler_attention = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_attention, mode='min', factor=0.1, patience=5) # Adjust patience as needed\n","\n","    if device.type == \"cuda\":\n","        torch.cuda.empty_cache()\n","        print(f\"Initial GPU memory allocated for Attention U-Net: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        # Training phase\n","        epoch_loss_attention = train_one_epoch(model_attention, train_loader, optimizer_attention, criterion, device, \"Attention U-Net\")\n","        if epoch_loss_attention == float('inf'):\n","            print(f\"Critical error training Attention U-Net in epoch {epoch+1}. Stopping training for this model.\")\n","            break\n","        print(f\"  Attention U-Net - Epoch {epoch+1} Average Training Loss: {epoch_loss_attention:.4f}\")\n","\n","        # Validation phase\n","        if val_loader:\n","            current_val_loss = evaluate_validation_loss(model_attention, val_loader, criterion, device, \"Attention U-Net (Validation)\")\n","            print(f\"  Attention U-Net - Epoch {epoch+1} Average Validation Loss: {current_val_loss:.4f}\")\n","\n","            if current_val_loss < best_val_loss_attention:\n","                best_val_loss_attention = current_val_loss\n","                torch.save(model_attention.state_dict(), best_model_attention_path)\n","                print(f\"    New best model saved to {best_model_attention_path} (Val Loss: {current_val_loss:.4f})\")\n","\n","            # Step the learning rate scheduler based on validation loss\n","            scheduler_attention.step(current_val_loss)\n","        else:\n","            print(\"  Skipping validation for checkpointing and LR scheduling as val_loader is not available.\")\n","\n","    # Save the final model state\n","    torch.save(model_attention.state_dict(), final_model_attention_path)\n","    print(f\"--- Attention U-Net Training Finished. Final model saved to {final_model_attention_path} ---\")\n","    if val_loader and os.path.exists(best_model_attention_path):\n","        print(f\"Best validation model saved at {best_model_attention_path} with val_loss: {best_val_loss_attention:.4f}\")\n","else:\n","    print(\"Skipping Attention U-Net training as train_loader is not available.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ryEq-i6BNJg","outputId":"1ef43b13-aadd-4a36-f4fd-b0043078b6d3","executionInfo":{"status":"ok","timestamp":1747308964178,"user_tz":300,"elapsed":1966484,"user":{"displayName":"mk chung","userId":"05412849786518098631"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Fine-tuning Attention U-Net (SCSE, mobilenet_v2) ---\n","Using existing 'model_attention' instance.\n","Loading weights from /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_mobilenetv2.pth for fine-tuning...\n","Successfully loaded pre-trained weights.\n","Initial GPU memory allocated for Fine-tuning Attention U-Net: 107.02 MB\n","Starting fine-tuning for 5 epochs with LR: 5e-05\n","Fine-tuning Epoch 1/5. Current LR: 5.00e-05\n","    Attention U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.1865\n","    Attention U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.2939\n","    Attention U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.3239\n","    Attention U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.2118\n","    Attention U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.3603\n","  Attention U-Net (Fine-tuning) - Epoch 1 Average Training Loss: 0.2721\n","  Attention U-Net (Fine-tuning) - Epoch 1 Average Validation Loss: 0.3415\n","    New best fine-tuned model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_ft_best_val_mobilenetv2.pth (Val Loss: 0.3415)\n","Fine-tuning Epoch 2/5. Current LR: 5.00e-05\n","    Attention U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.1769\n","    Attention U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.2836\n","    Attention U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.2149\n","    Attention U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.1910\n","    Attention U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.2716\n","  Attention U-Net (Fine-tuning) - Epoch 2 Average Training Loss: 0.2621\n","  Attention U-Net (Fine-tuning) - Epoch 2 Average Validation Loss: 0.3372\n","    New best fine-tuned model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_ft_best_val_mobilenetv2.pth (Val Loss: 0.3372)\n","Fine-tuning Epoch 3/5. Current LR: 5.00e-05\n","    Attention U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.2101\n","    Attention U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.4531\n","    Attention U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.1715\n","    Attention U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.3125\n","    Attention U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.2518\n","  Attention U-Net (Fine-tuning) - Epoch 3 Average Training Loss: 0.2566\n","  Attention U-Net (Fine-tuning) - Epoch 3 Average Validation Loss: 0.3364\n","    New best fine-tuned model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_ft_best_val_mobilenetv2.pth (Val Loss: 0.3364)\n","Fine-tuning Epoch 4/5. Current LR: 5.00e-05\n","    Attention U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.2410\n","    Attention U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.3631\n","    Attention U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.3171\n","    Attention U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.1744\n","    Attention U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.2121\n","  Attention U-Net (Fine-tuning) - Epoch 4 Average Training Loss: 0.2526\n","  Attention U-Net (Fine-tuning) - Epoch 4 Average Validation Loss: 0.3358\n","    New best fine-tuned model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_ft_best_val_mobilenetv2.pth (Val Loss: 0.3358)\n","Fine-tuning Epoch 5/5. Current LR: 5.00e-05\n","    Attention U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.3333\n","    Attention U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.2968\n","    Attention U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.2054\n","    Attention U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.3965\n","    Attention U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.2358\n","  Attention U-Net (Fine-tuning) - Epoch 5 Average Training Loss: 0.2501\n","  Attention U-Net (Fine-tuning) - Epoch 5 Average Validation Loss: 0.3331\n","    New best fine-tuned model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_ft_best_val_mobilenetv2.pth (Val Loss: 0.3331)\n","--- Attention U-Net Fine-tuning Finished. Final model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_ft_final_epoch_mobilenetv2.pth ---\n","Best fine-tuned validation model saved at /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_ft_best_val_mobilenetv2.pth with val_loss: 0.3331\n"]}],"source":["# --- Training Cell 9.2.1: Fine-tune Attention U-Net ---\n","print(f\"\\n--- Fine-tuning Attention U-Net (SCSE, {encoder_name}) ---\")\n","\n","# Define parameters for fine-tuning\n","num_ft_epochs = 5\n","fine_tune_lr = 5e-5\n","\n","# Paths for the fine-tuned model\n","model_name_attention_ft = \"attention_unet_scse_ft\"\n","best_model_attention_ft_path = os.path.join(save_dir, f\"{model_name_attention_ft}_best_val_mobilenetv2.pth\")\n","final_model_attention_ft_path = os.path.join(save_dir, f\"{model_name_attention_ft}_final_epoch_mobilenetv2.pth\")\n","\n","pretrained_model_path = os.path.join(save_dir, \"attention_unet_scse_best_val_mobilenetv2.pth\")\n","\n","best_val_loss_attention_ft = 0.4018\n","\n","if not os.path.exists(pretrained_model_path):\n","    print(f\"Error: Pretrained model not found at {pretrained_model_path}. Cannot fine-tune.\")\n","    print(\"Please ensure the initial training (Cell 9.2) was run and the best model was saved correctly.\")\n","else:\n","    if train_loader and val_loader:\n","        if 'model_attention' not in globals() or model_attention is None:\n","            print(\"Re-defining Attention U-Net model architecture for fine-tuning...\")\n","            model_attention = smp.Unet(\n","                encoder_name=encoder_name,\n","                encoder_weights=None,\n","                in_channels=3,\n","                classes=num_classes,\n","                activation=None,\n","                decoder_attention_type=\"scse\"\n","            )\n","            print(\"Attention U-Net model re-defined.\")\n","        else:\n","            print(\"Using existing 'model_attention' instance.\")\n","\n","\n","        print(f\"Loading weights from {pretrained_model_path} for fine-tuning...\")\n","        try:\n","            model_attention.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n","            print(\"Successfully loaded pre-trained weights.\")\n","        except Exception as e:\n","            print(f\"Error loading pre-trained weights: {e}\")\n","            raise\n","\n","        model_attention.to(device)\n","\n","        optimizer_attention_ft = torch.optim.Adam(model_attention.parameters(), lr=fine_tune_lr, weight_decay=weight_decay)\n","\n","        scheduler_attention_ft = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_attention_ft, mode='min', factor=0.1, patience=3)\n","\n","        if device.type == \"cuda\":\n","            torch.cuda.empty_cache()\n","            print(f\"Initial GPU memory allocated for Fine-tuning Attention U-Net: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n","\n","        print(f\"Starting fine-tuning for {num_ft_epochs} epochs with LR: {fine_tune_lr}\")\n","        for epoch in range(num_ft_epochs):\n","            current_epoch_ft = epoch + 1\n","            print(f\"Fine-tuning Epoch {current_epoch_ft}/{num_ft_epochs}. Current LR: {optimizer_attention_ft.param_groups[0]['lr']:.2e}\")\n","\n","            epoch_loss_attention_ft = train_one_epoch(model_attention, train_loader, optimizer_attention_ft, criterion, device, \"Attention U-Net (Fine-tuning)\")\n","            if epoch_loss_attention_ft == float('inf'):\n","                print(f\"Critical error fine-tuning Attention U-Net in epoch {current_epoch_ft}. Stopping fine-tuning.\")\n","                break\n","            print(f\"  Attention U-Net (Fine-tuning) - Epoch {current_epoch_ft} Average Training Loss: {epoch_loss_attention_ft:.4f}\")\n","\n","            current_val_loss_ft = evaluate_validation_loss(model_attention, val_loader, criterion, device, \"Attention U-Net (Fine-tuning Validation)\")\n","            print(f\"  Attention U-Net (Fine-tuning) - Epoch {current_epoch_ft} Average Validation Loss: {current_val_loss_ft:.4f}\")\n","\n","            if current_val_loss_ft < best_val_loss_attention_ft:\n","                best_val_loss_attention_ft = current_val_loss_ft\n","                torch.save(model_attention.state_dict(), best_model_attention_ft_path)\n","                print(f\"    New best fine-tuned model saved to {best_model_attention_ft_path} (Val Loss: {current_val_loss_ft:.4f})\")\n","\n","            scheduler_attention_ft.step(current_val_loss_ft)\n","\n","        torch.save(model_attention.state_dict(), final_model_attention_ft_path)\n","        print(f\"--- Attention U-Net Fine-tuning Finished. Final model saved to {final_model_attention_ft_path} ---\")\n","        if os.path.exists(best_model_attention_ft_path):\n","            print(f\"Best fine-tuned validation model saved at {best_model_attention_ft_path} with val_loss: {best_val_loss_attention_ft:.4f}\")\n","    else:\n","        if not train_loader: print(\"Skipping fine-tuning as train_loader is not available.\")\n","        if not val_loader: print(\"Skipping fine-tuning as val_loader is not available (validation is crucial for fine-tuning).\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1_aQzJ_fR-AcQcVYYLwxgZ5gvtN4h2M6a","timestamp":1747251036282}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}