{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"CAMAFfnpeUBv","outputId":"cf0bae05-dcd2-4aa1-af4b-708b6b30e4ec","collapsed":true,"executionInfo":{"status":"ok","timestamp":1747283853947,"user_tz":300,"elapsed":253437,"user":{"displayName":"mk chung","userId":"05412849786518098631"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch\n","  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n","Collecting filelock (from torch)\n","  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting typing-extensions>=4.10.0 (from torch)\n","  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n","Collecting sympy>=1.13.3 (from torch)\n","  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n","Collecting networkx (from torch)\n","  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n","Collecting jinja2 (from torch)\n","  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n","Collecting fsspec (from torch)\n","  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n","  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n","  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n","  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n","  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n","  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n","  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.3.0 (from torch)\n","  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n","Collecting setuptools>=40.8.0 (from triton==3.3.0->torch)\n","  Downloading setuptools-80.7.1-py3-none-any.whl.metadata (6.6 kB)\n","Collecting numpy (from torchvision)\n","  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n","  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n","Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n","  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n","Collecting MarkupSafe>=2.0 (from jinja2->torch)\n","  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n","Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n","Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n","Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading setuptools-80.7.1-py3-none-any.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, setuptools, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision\n","  Attempting uninstall: nvidia-cusparselt-cu12\n","    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n","    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n","      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n","  Attempting uninstall: mpmath\n","    Found existing installation: mpmath 1.3.0\n","    Uninstalling mpmath-1.3.0:\n","      Successfully uninstalled mpmath-1.3.0\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.13.2\n","    Uninstalling typing_extensions-4.13.2:\n","      Successfully uninstalled typing_extensions-4.13.2\n","  Attempting uninstall: sympy\n","    Found existing installation: sympy 1.13.1\n","    Uninstalling sympy-1.13.1:\n","      Successfully uninstalled sympy-1.13.1\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 75.2.0\n","    Uninstalling setuptools-75.2.0:\n","      Successfully uninstalled setuptools-75.2.0\n","  Attempting uninstall: pillow\n","    Found existing installation: pillow 11.2.1\n","    Uninstalling pillow-11.2.1:\n","      Successfully uninstalled pillow-11.2.1\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.4.127\n","    Uninstalling nvidia-nvtx-cu12-12.4.127:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.21.5\n","    Uninstalling nvidia-nccl-cu12-2.21.5:\n","      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: networkx\n","    Found existing installation: networkx 3.4.2\n","    Uninstalling networkx-3.4.2:\n","      Successfully uninstalled networkx-3.4.2\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 3.0.2\n","    Uninstalling MarkupSafe-3.0.2:\n","      Successfully uninstalled MarkupSafe-3.0.2\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","  Attempting uninstall: filelock\n","    Found existing installation: filelock 3.18.0\n","    Uninstalling filelock-3.18.0:\n","      Successfully uninstalled filelock-3.18.0\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.2.0\n","    Uninstalling triton-3.2.0:\n","      Successfully uninstalled triton-3.2.0\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: jinja2\n","    Found existing installation: Jinja2 3.1.6\n","    Uninstalling Jinja2-3.1.6:\n","      Successfully uninstalled Jinja2-3.1.6\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.6.0+cu124\n","    Uninstalling torch-2.6.0+cu124:\n","      Successfully uninstalled torch-2.6.0+cu124\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.21.0+cu124\n","    Uninstalling torchvision-0.21.0+cu124:\n","      Successfully uninstalled torchvision-0.21.0+cu124\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n","fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.3.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.5 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 pillow-11.2.1 setuptools-80.7.1 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 triton-3.3.0 typing-extensions-4.13.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","_distutils_hack","pkg_resources"]},"id":"79dce5c5ff994721aac24a195aa4bea1"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting segmentation-models-pytorch\n","  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.31.1)\n","Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.2.5)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.2.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\n","Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.15)\n","Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.7.0)\n","Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.22.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.13.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.5.1.17)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.3)\n","Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.26.2)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.11.1.6)\n","Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.3.0)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch>=1.8->segmentation-models-pytorch) (80.7.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.4.26)\n","Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: segmentation-models-pytorch\n","Successfully installed segmentation-models-pytorch-0.5.0\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.2.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"]}],"source":["# CS444 U-Net Project Setup for Semantic Segmentation\n","# Attention U-Net (ResNet50) on Cityscapes dataset\n","\n","# Step 1: Install Required Libraries\n","!pip install torch torchvision --force-reinstall\n","!pip install segmentation-models-pytorch\n","!pip install matplotlib # Ensure matplotlib is installed for visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6ZJKRlgeaD3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747284068380,"user_tz":300,"elapsed":8639,"user":{"displayName":"mk chung","userId":"05412849786518098631"}},"outputId":"67076671-5bcf-4c59-8369-e2a49b8e9230"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Step 2: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["ls '/content/drive/MyDrive/U-Net Segmentation Project/'"],"metadata":{"id":"9dTNH29BVUCZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZZIn2y9ecdi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747284132588,"user_tz":300,"elapsed":1385,"user":{"displayName":"mk chung","userId":"05412849786518098631"}},"outputId":"43b2d3e4-de03-428e-dda5-a553fe282c98"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset structure verified: 'leftImg8bit' and 'gtFine' found in /content/drive/MyDrive/U-Net Segmentation Project/CityScapes.\n"]}],"source":["# Step 3: Verify Dataset Structure\n","import os\n","\n","# IMPORTANT: Update this path to your specific Google Drive location for the Cityscapes dataset\n","root_path = '/content/drive/MyDrive/U-Net Segmentation Project/CityScapes'\n","\n","if not (os.path.exists(os.path.join(root_path, 'leftImg8bit')) and os.path.exists(os.path.join(root_path, 'gtFine'))):\n","    raise RuntimeError(f\"Ensure 'leftImg8bit' and 'gtFine' folders are in the root directory: {root_path}. Please verify the path.\")\n","else:\n","    print(f\"Dataset structure verified: 'leftImg8bit' and 'gtFine' found in {root_path}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3325,"status":"ok","timestamp":1747284147255,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"_pz2bfSLefC2","outputId":"c19c3e9c-f708-40e8-f5c1-d525b31622af"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.7.0+cu126\n","CUDA available: True\n","CUDA version: 12.6\n","GPU device: Tesla T4\n"]}],"source":["# Step 4: Verify GPU Environment\n","import torch\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"CUDA version: {torch.version.cuda}\")\n","    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n","    device = torch.device(\"cuda\")\n","else:\n","    print(\"No GPU detected. Training will run on CPU, which will be very slow. Ensure GPU runtime is enabled in Colab (Runtime > Change runtime type).\")\n","    device = torch.device(\"cpu\")\n","\n","# Set environment variables for CUDA debugging\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Synchronous CUDA errors\n","os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"    # Device-side assertions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11099,"status":"ok","timestamp":1747284159196,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"qvwy07W_ehly","outputId":"6e8613ea-eda2-4481-9913-e5a176460182"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded train dataset with 2975 samples.\n","Successfully loaded val dataset with 500 samples.\n","Sample image shape: torch.Size([3, 128, 256])\n","Sample target shape: torch.Size([128, 256])\n","Sample target unique values: tensor([  0,   1,   2,   5,   7,   8,   9,  10,  11,  13,  18, 255])\n"]}],"source":["# Step 5: Load Cityscapes Dataset\n","from torchvision.datasets import Cityscapes\n","from torchvision import transforms\n","import numpy as np\n","from PIL import Image # Needed for target resize resampling\n","\n","# Define transforms (resize to manage memory)\n","# Original image size for Cityscapes is 1024x2048. Resizing significantly.\n","img_height, img_width = 128, 256 # Reduced size for faster training and lower memory\n","\n","transform = transforms.Compose([\n","    transforms.Resize((img_height, img_width)),\n","    transforms.ToTensor()\n","])\n","\n","def target_to_tensor(target):\n","    # Resize target segmentation map. PIL Image object is expected by Resize.\n","    target = target.resize((img_width, img_height), resample=Image.NEAREST) # Nearest neighbor for labels\n","    target_np = np.array(target, dtype=np.uint8)\n","\n","    # Map Cityscapes labels to 0-18 (for 19 classes), set others to ignore_index (255)\n","    label_map = {\n","        # name: id, trainId\n","        'unlabeled': (0, 255), 'ego vehicle': (1, 255), 'rectification border': (2, 255),\n","        'out of roi': (3, 255), 'static': (4, 255), 'dynamic': (5, 255),\n","        'ground': (6, 255), 'road': (7, 0), 'sidewalk': (8, 1),\n","        'parking': (9, 255), 'rail track': (10, 255), 'building': (11, 2),\n","        'wall': (12, 3), 'fence': (13, 4), 'guard rail': (14, 255),\n","        'bridge': (15, 255), 'tunnel': (16, 255), 'pole': (17, 5),\n","        'polegroup': (18, 255), 'traffic light': (19, 6), 'traffic sign': (20, 7),\n","        'vegetation': (21, 8), 'terrain': (22, 9), 'sky': (23, 10),\n","        'person': (24, 11), 'rider': (25, 12), 'car': (26, 13),\n","        'truck': (27, 14), 'bus': (28, 15), 'caravan': (29, 255),\n","        'trailer': (30, 255), 'train': (31, 16), 'motorcycle': (32, 17),\n","        'bicycle': (33, 18)\n","    }\n","    # Create an empty array with ignore_index\n","    mapped_target = np.full_like(target_np, 255, dtype=np.uint8) # 255 for ignore_index\n","\n","    for cityscapes_id_tuple, train_id in label_map.items():\n","        original_id = train_id[0] # Using the 'id' from the tuple for mapping\n","        target_train_id = train_id[1]\n","        if target_train_id != 255: # If it's a valid trainId\n","            mapped_target[target_np == original_id] = target_train_id\n","\n","    return torch.from_numpy(mapped_target).long()\n","\n","# Load training and validation datasets\n","try:\n","    train_dataset = Cityscapes(\n","        root=root_path,\n","        split='train',\n","        mode='fine',\n","        target_type='semantic',\n","        transform=transform,\n","        target_transform=target_to_tensor\n","    )\n","    val_dataset = Cityscapes(\n","        root=root_path,\n","        split='val',\n","        mode='fine',\n","        target_type='semantic',\n","        transform=transform,\n","        target_transform=target_to_tensor\n","    )\n","    print(f\"Successfully loaded train dataset with {len(train_dataset)} samples.\")\n","    print(f\"Successfully loaded val dataset with {len(val_dataset)} samples.\")\n","\n","except Exception as e:\n","    print(f\"Error loading dataset: {e}\")\n","    print(\"Please ensure your `root_path` is correct and the dataset is properly structured.\")\n","    raise\n","\n","# Verify dataset labels from a sample\n","if len(train_dataset) > 0:\n","    sample_image, sample_target = train_dataset[0]\n","    print(f\"Sample image shape: {sample_image.shape}\")\n","    print(f\"Sample target shape: {sample_target.shape}\")\n","    unique_labels = torch.unique(sample_target)\n","    print(f\"Sample target unique values: {unique_labels}\")\n","    if not (all( (unique_labels >= 0) & (unique_labels <= 18) | (unique_labels == 255) )):\n","        print(\"Warning: Unexpected label values found in sample target.\")\n","else:\n","    print(\"Train dataset is empty. Cannot verify sample.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15281,"status":"ok","timestamp":1747284201919,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"wcmRHoRUellz","outputId":"743c04b6-02a5-4622-b07c-1a076585a800"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking train dataset integrity (first 5 samples)...\n","train dataset integrity verified for first 5 samples.\n","Checking val dataset integrity (first 5 samples)...\n","val dataset integrity verified for first 5 samples.\n"]}],"source":["# Step 5.1: Verify Dataset Integrity\n","def check_dataset_integrity(dataset, split_name, num_samples_to_check=5):\n","    print(f\"Checking {split_name} dataset integrity (first {num_samples_to_check} samples)...\")\n","    if len(dataset) == 0:\n","        print(f\"{split_name} dataset is empty. Skipping integrity check.\")\n","        return True\n","    for i in range(min(num_samples_to_check, len(dataset))):\n","        try:\n","            image, target = dataset[i]\n","            # Basic checks\n","            if not isinstance(image, torch.Tensor) or not isinstance(target, torch.Tensor):\n","                print(f\"Error at index {i}: Image or target is not a tensor.\")\n","                return False\n","            if image.shape != torch.Size([3, img_height, img_width]):\n","                 print(f\"Error at index {i}: Unexpected image shape {image.shape}.\")\n","                 return False\n","            if target.shape != torch.Size([img_height, img_width]):\n","                 print(f\"Error at index {i}: Unexpected target shape {target.shape}.\")\n","                 return False\n","            if not ((target >= 0) & (target <= 18) | (target == 255)).all():\n","                 print(f\"Error at index {i}: Target contains invalid labels {torch.unique(target)}.\")\n","                 return False\n","\n","        except Exception as e:\n","            print(f\"Error accessing sample {i} in {split_name} dataset: {e}\")\n","            return False\n","    print(f\"{split_name} dataset integrity verified for first {num_samples_to_check} samples.\")\n","    return True\n","\n","# Check train and validation datasets (first few samples)\n","if not check_dataset_integrity(train_dataset, \"train\"):\n","    raise RuntimeError(\"Train dataset integrity check failed.\")\n","if not check_dataset_integrity(val_dataset, \"val\"):\n","    raise RuntimeError(\"Validation dataset integrity check failed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1747284201920,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"4wQ7hLvpeogt","outputId":"5ef1d054-17e0-45ef-8220-1b81eaa75e89"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train loader created with batch size 2.\n","Validation loader created with batch size 2.\n"]}],"source":["# Step 6: Create Data Loaders\n","from torch.utils.data import DataLoader\n","\n","batch_size = 2\n","\n","if len(train_dataset) > 0:\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","else:\n","    train_loader = None\n","    print(\"Train dataset is empty. Train loader not created.\")\n","\n","if len(val_dataset) > 0:\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","else:\n","    val_loader = None\n","    print(\"Validation dataset is empty. Validation loader not created.\")\n","\n","if train_loader:\n","    print(f\"Train loader created with batch size {batch_size}.\")\n","if val_loader:\n","    print(f\"Validation loader created with batch size {batch_size}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237,"referenced_widgets":["2e58bdb6c19149b39685f37c91d3c94c","39d0f1f795aa4744a87edf446d72e9c9","7f1ded5990ad4ed4a1097798e8c08b82","49d6b252e3d44883b587afaf8589f593","9913c4c80e3a473794c6dc8d52d1b465","bb8a71616c6041f3960cd92d9a7c9ae8","61abbc89cc114cd49b238e4be25e144f","f8899cd68a83448b8ed1a5ea1852b55c","c2ce27dcaddd48e6ace7861d796b5b86","0b4eefc4d6d24652881b77cb240a92ed","27769240574d40eab11945bda3480170","baa9e8a9156648aab10d3b24bfac4e8a","dd577436993e4107a6f817d18a0b3693","d135885ebecc4debbc8167ffac3e1fba","35152a98c4a54085aef82285eb8e3d02","86daa8cdfda24bf2ab14c9bd43dc60c7","be236d8207ec4d70b4fcc10aa5f2c5bf","c0a161c5d6ce4faaa3460515643ed231","98112dd2b0db49ec8f1983f20a5b75f5","81b4bbeab6fa48b29e517e330da1bdfa","97ebcf5389b641f3a8eb34cdf9f99c9a","2fc4db2529ac44d18496c01bf1618557"]},"executionInfo":{"elapsed":8096,"status":"ok","timestamp":1747284210013,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"E07uINSEerSA","outputId":"1442bba1-7cee-4466-dc64-97aaf21b0651"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e58bdb6c19149b39685f37c91d3c94c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baa9e8a9156648aab10d3b24bfac4e8a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Standard U-Net with resnet50 encoder defined.\n","Attention U-Net (SCSE) with resnet50 encoder defined.\n","U-Net++ with resnet50 encoder defined.\n"]}],"source":["# Step 7: Define Models\n","import segmentation_models_pytorch as smp\n","\n","num_classes = 19  # Based on the label mapping (0-18)\n","encoder_name = \"resnet50\"\n","\n","# Attention U-Net (with SCSE attention)\n","model_attention = smp.Unet(\n","    encoder_name=encoder_name,\n","    encoder_weights=\"imagenet\",\n","    in_channels=3,\n","    classes=num_classes,\n","    activation=None,\n","    decoder_attention_type=\"scse\" # Spatial and Channel Squeeze & Excitation\n",")\n","print(f\"Attention U-Net (SCSE) with {encoder_name} encoder defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1747284210031,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"UsTMZjp8eubU","outputId":"95aa3c0b-42d6-4566-c5b6-208455fb2d25"},"outputs":[{"output_type":"stream","name":"stdout","text":["CrossEntropyLoss defined with ignore_index=255.\n"]}],"source":["# Step 8: Define Loss Function\n","import torch.nn as nn\n","# CrossEntropyLoss expects raw logits from the model and long type targets.\n","# ignore_index=255 means that pixels with label 255 will not contribute to the loss.\n","criterion = nn.CrossEntropyLoss(ignore_index=255)\n","print(\"CrossEntropyLoss defined with ignore_index=255.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1747284210083,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"tn_l-Gsvg6El","outputId":"3339ba77-b964-44e2-8a05-84ece25301fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Models will be saved in: /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models\n"]}],"source":["# Step 8.1: Setup Model Saving and Validation Loss Function\n","import os\n","\n","save_dir = os.path.join(root_path, 'saved_models')\n","os.makedirs(save_dir, exist_ok=True)\n","print(f\"Models will be saved in: {save_dir}\")\n","\n","def evaluate_validation_loss(model, loader, criterion, device, model_name):\n","    model.eval() # Set model to evaluation mode\n","    running_val_loss = 0.0\n","    num_batches = len(loader)\n","\n","    if num_batches == 0:\n","        print(f\"Warning: Validation loader for {model_name} is empty. Cannot compute validation loss.\")\n","        return float('inf') # Return infinity if no validation can be done\n","\n","    with torch.no_grad(): # No gradients needed for validation\n","        for images, targets in loader:\n","            images, targets = images.to(device), targets.to(device)\n","\n","            # Ensure targets are valid before calculating loss\n","            if not ((targets >= 0) & (targets < num_classes) | (targets == 255)).all():\n","                print(f\"Validation: Invalid labels detected in targets for {model_name}! Unique: {torch.unique(targets)}. Skipping batch for loss calculation.\")\n","                pass\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, targets)\n","            running_val_loss += loss.item()\n","\n","    avg_val_loss = running_val_loss / num_batches if num_batches > 0 else float('inf')\n","    if device.type == \"cuda\":\n","        torch.cuda.empty_cache()\n","    return avg_val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsHCbdHPexJ-"},"outputs":[],"source":["# Step 9: Training Configuration\n","num_epochs = 15\n","learning_rate = 0.001\n","weight_decay = 1e-5\n","\n","def train_one_epoch(model, loader, optimizer, criterion, device, model_name):\n","    model.train()\n","    running_loss = 0.0\n","    num_batches = len(loader)\n","\n","    if num_batches == 0:\n","        print(f\"Warning: Training loader for {model_name} is empty. Skipping training epoch.\")\n","        return 0.0\n","\n","    for i, (images, targets) in enumerate(loader):\n","        images, targets = images.to(device), targets.to(device)\n","\n","        try:\n","            # Verify target labels before passing to model/loss\n","            if not ((targets >= 0) & (targets < num_classes) | (targets == 255)).all():\n","                print(f\"Batch {i}: Invalid labels detected in targets for {model_name}! Unique: {torch.unique(targets)}. Skipping batch.\")\n","                problematic_targets = targets.clone()\n","                problematic_targets[(targets >= 0) & (targets < num_classes) | (targets == 255)] = -1 # Mark valid ones\n","                print(f\"Problematic target values: {torch.unique(problematic_targets.masked_select(problematic_targets != -1))}\")\n","                continue # Skip this batch\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","            if (i + 1) % (num_batches // 5 if num_batches >= 5 else 1) == 0: # Print 5 times per epoch\n","                print(f\"    {model_name} - Batch {i+1}/{num_batches}, Loss: {loss.item():.4f}\")\n","\n","        except RuntimeError as e:\n","            print(f\"RuntimeError during training {model_name} at batch {i}: {e}\")\n","            if \"CUDA out of memory\" in str(e):\n","                print(\"CUDA OOM: Try reducing batch size or image dimensions.\")\n","                if device.type == \"cuda\": torch.cuda.empty_cache()\n","            return float('inf') # Indicate critical error\n","\n","    epoch_loss = running_loss / num_batches if num_batches > 0 else 0\n","    if device.type == \"cuda\":\n","        torch.cuda.empty_cache()\n","    return epoch_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7981901,"status":"ok","timestamp":1747292400838,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"i3cRNapne1wP","outputId":"d2108be2-4769-4d30-af19-b7e92504d099","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Training Attention U-Net (SCSE, resnet50) ---\n","Initial GPU memory allocated for Attention U-Net: 132.03 MB\n","Epoch 1/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.6075\n","    Attention U-Net - Batch 594/1488, Loss: 0.6585\n","    Attention U-Net - Batch 891/1488, Loss: 0.3724\n","    Attention U-Net - Batch 1188/1488, Loss: 0.5450\n","    Attention U-Net - Batch 1485/1488, Loss: 0.4973\n","  Attention U-Net - Epoch 1 Average Training Loss: 0.6767\n","  Attention U-Net - Epoch 1 Average Validation Loss: 0.6061\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_resnet50.pth (Val Loss: 0.6061)\n","Epoch 2/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.3425\n","    Attention U-Net - Batch 594/1488, Loss: 0.4459\n","    Attention U-Net - Batch 891/1488, Loss: 0.3759\n","    Attention U-Net - Batch 1188/1488, Loss: 0.3397\n","    Attention U-Net - Batch 1485/1488, Loss: 0.8056\n","  Attention U-Net - Epoch 2 Average Training Loss: 0.5089\n","  Attention U-Net - Epoch 2 Average Validation Loss: 0.5303\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_resnet50.pth (Val Loss: 0.5303)\n","Epoch 3/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.5359\n","    Attention U-Net - Batch 594/1488, Loss: 0.8617\n","    Attention U-Net - Batch 891/1488, Loss: 0.6167\n","    Attention U-Net - Batch 1188/1488, Loss: 0.3498\n","    Attention U-Net - Batch 1485/1488, Loss: 0.3338\n","  Attention U-Net - Epoch 3 Average Training Loss: 0.4573\n","  Attention U-Net - Epoch 3 Average Validation Loss: 0.9991\n","Epoch 4/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.4016\n","    Attention U-Net - Batch 594/1488, Loss: 0.3201\n","    Attention U-Net - Batch 891/1488, Loss: 0.3448\n","    Attention U-Net - Batch 1188/1488, Loss: 0.3161\n","    Attention U-Net - Batch 1485/1488, Loss: 0.2376\n","  Attention U-Net - Epoch 4 Average Training Loss: 0.4318\n","  Attention U-Net - Epoch 4 Average Validation Loss: 0.5720\n","Epoch 5/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.4152\n","    Attention U-Net - Batch 594/1488, Loss: 0.3877\n","    Attention U-Net - Batch 891/1488, Loss: 0.6802\n","    Attention U-Net - Batch 1188/1488, Loss: 0.3919\n","    Attention U-Net - Batch 1485/1488, Loss: 0.2571\n","  Attention U-Net - Epoch 5 Average Training Loss: 0.4055\n","  Attention U-Net - Epoch 5 Average Validation Loss: 0.5085\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_resnet50.pth (Val Loss: 0.5085)\n","Epoch 6/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.3399\n","    Attention U-Net - Batch 594/1488, Loss: 0.2765\n","    Attention U-Net - Batch 891/1488, Loss: 0.4269\n","    Attention U-Net - Batch 1188/1488, Loss: 0.3305\n","    Attention U-Net - Batch 1485/1488, Loss: 0.4155\n","  Attention U-Net - Epoch 6 Average Training Loss: 0.3890\n","  Attention U-Net - Epoch 6 Average Validation Loss: 0.4656\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_resnet50.pth (Val Loss: 0.4656)\n","Epoch 7/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.6000\n","    Attention U-Net - Batch 594/1488, Loss: 0.5217\n","    Attention U-Net - Batch 891/1488, Loss: 0.3687\n","    Attention U-Net - Batch 1188/1488, Loss: 0.3807\n","    Attention U-Net - Batch 1485/1488, Loss: 0.2926\n","  Attention U-Net - Epoch 7 Average Training Loss: 0.3774\n","  Attention U-Net - Epoch 7 Average Validation Loss: 0.4289\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_resnet50.pth (Val Loss: 0.4289)\n","Epoch 8/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.5975\n","    Attention U-Net - Batch 594/1488, Loss: 0.2953\n","    Attention U-Net - Batch 891/1488, Loss: 0.3311\n","    Attention U-Net - Batch 1188/1488, Loss: 0.2675\n","    Attention U-Net - Batch 1485/1488, Loss: 0.3998\n","  Attention U-Net - Epoch 8 Average Training Loss: 0.3635\n","  Attention U-Net - Epoch 8 Average Validation Loss: 0.4169\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_resnet50.pth (Val Loss: 0.4169)\n","Epoch 9/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.2977\n","    Attention U-Net - Batch 594/1488, Loss: 0.3732\n","    Attention U-Net - Batch 891/1488, Loss: 0.2257\n","    Attention U-Net - Batch 1188/1488, Loss: 0.4250\n","    Attention U-Net - Batch 1485/1488, Loss: 0.2275\n","  Attention U-Net - Epoch 9 Average Training Loss: 0.3557\n","  Attention U-Net - Epoch 9 Average Validation Loss: 0.5074\n","Epoch 10/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.2921\n","    Attention U-Net - Batch 594/1488, Loss: 0.2596\n","    Attention U-Net - Batch 891/1488, Loss: 0.2986\n","    Attention U-Net - Batch 1188/1488, Loss: 0.3576\n","    Attention U-Net - Batch 1485/1488, Loss: 0.2332\n","  Attention U-Net - Epoch 10 Average Training Loss: 0.3408\n","  Attention U-Net - Epoch 10 Average Validation Loss: 0.5111\n","Epoch 11/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.4063\n","    Attention U-Net - Batch 594/1488, Loss: 0.3552\n","    Attention U-Net - Batch 891/1488, Loss: 0.3163\n","    Attention U-Net - Batch 1188/1488, Loss: 0.6069\n","    Attention U-Net - Batch 1485/1488, Loss: 0.5784\n","  Attention U-Net - Epoch 11 Average Training Loss: 0.3363\n","  Attention U-Net - Epoch 11 Average Validation Loss: 0.4086\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_resnet50.pth (Val Loss: 0.4086)\n","Epoch 12/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.2127\n","    Attention U-Net - Batch 594/1488, Loss: 0.4252\n","    Attention U-Net - Batch 891/1488, Loss: 0.3138\n","    Attention U-Net - Batch 1188/1488, Loss: 0.4218\n","    Attention U-Net - Batch 1485/1488, Loss: 0.4044\n","  Attention U-Net - Epoch 12 Average Training Loss: 0.3284\n","  Attention U-Net - Epoch 12 Average Validation Loss: 0.4077\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_resnet50.pth (Val Loss: 0.4077)\n","Epoch 13/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.5271\n","    Attention U-Net - Batch 594/1488, Loss: 0.4094\n","    Attention U-Net - Batch 891/1488, Loss: 0.2475\n","    Attention U-Net - Batch 1188/1488, Loss: 0.2270\n","    Attention U-Net - Batch 1485/1488, Loss: 0.4669\n","  Attention U-Net - Epoch 13 Average Training Loss: 0.3202\n","  Attention U-Net - Epoch 13 Average Validation Loss: 0.4229\n","Epoch 14/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.2758\n","    Attention U-Net - Batch 594/1488, Loss: 0.3185\n","    Attention U-Net - Batch 891/1488, Loss: 0.3868\n","    Attention U-Net - Batch 1188/1488, Loss: 0.2119\n","    Attention U-Net - Batch 1485/1488, Loss: 1.6291\n","  Attention U-Net - Epoch 14 Average Training Loss: 0.3138\n","  Attention U-Net - Epoch 14 Average Validation Loss: 0.4515\n","Epoch 15/15\n","    Attention U-Net - Batch 297/1488, Loss: 0.1620\n","    Attention U-Net - Batch 594/1488, Loss: 0.1849\n","    Attention U-Net - Batch 891/1488, Loss: 0.2451\n","    Attention U-Net - Batch 1188/1488, Loss: 0.3364\n","    Attention U-Net - Batch 1485/1488, Loss: 0.2483\n","  Attention U-Net - Epoch 15 Average Training Loss: 0.3119\n","  Attention U-Net - Epoch 15 Average Validation Loss: 0.4318\n","--- Attention U-Net Training Finished. Final model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_final_epoch_resnet50.pth ---\n","Best validation model saved at /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_resnet50.pth with val_loss: 0.4077\n"]}],"source":["# --- Training Cell 9.2: Attention U-Net ---\n","print(f\"\\n--- Training Attention U-Net (SCSE, {encoder_name}) ---\")\n","model_name_attention = \"attention_unet_scse\"\n","best_model_attention_path = os.path.join(save_dir, f\"{model_name_attention}_best_val_resnet50.pth\")\n","final_model_attention_path = os.path.join(save_dir, f\"{model_name_attention}_final_epoch_resnet50.pth\")\n","best_val_loss_attention = float('inf')\n","\n","if train_loader:\n","    model_attention.to(device)\n","    # Initialize optimizer with weight decay\n","    optimizer_attention = torch.optim.Adam(model_attention.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","    # Initialize learning rate scheduler\n","    scheduler_attention = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_attention, mode='min', factor=0.1, patience=5)\n","\n","    if device.type == \"cuda\":\n","        torch.cuda.empty_cache()\n","        print(f\"Initial GPU memory allocated for Attention U-Net: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        # Training phase\n","        epoch_loss_attention = train_one_epoch(model_attention, train_loader, optimizer_attention, criterion, device, \"Attention U-Net\")\n","        if epoch_loss_attention == float('inf'):\n","            print(f\"Critical error training Attention U-Net in epoch {epoch+1}. Stopping training for this model.\")\n","            break\n","        print(f\"  Attention U-Net - Epoch {epoch+1} Average Training Loss: {epoch_loss_attention:.4f}\")\n","\n","        # Validation phase\n","        if val_loader:\n","            current_val_loss = evaluate_validation_loss(model_attention, val_loader, criterion, device, \"Attention U-Net (Validation)\")\n","            print(f\"  Attention U-Net - Epoch {epoch+1} Average Validation Loss: {current_val_loss:.4f}\")\n","\n","            if current_val_loss < best_val_loss_attention:\n","                best_val_loss_attention = current_val_loss\n","                torch.save(model_attention.state_dict(), best_model_attention_path)\n","                print(f\"    New best model saved to {best_model_attention_path} (Val Loss: {current_val_loss:.4f})\")\n","\n","            # Step the learning rate scheduler based on validation loss\n","            scheduler_attention.step(current_val_loss)\n","        else:\n","            print(\"  Skipping validation for checkpointing and LR scheduling as val_loader is not available.\")\n","\n","    # Save the final model state\n","    torch.save(model_attention.state_dict(), final_model_attention_path)\n","    print(f\"--- Attention U-Net Training Finished. Final model saved to {final_model_attention_path} ---\")\n","    if val_loader and os.path.exists(best_model_attention_path):\n","        print(f\"Best validation model saved at {best_model_attention_path} with val_loss: {best_val_loss_attention:.4f}\")\n","else:\n","    print(\"Skipping Attention U-Net training as train_loader is not available.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ryEq-i6BNJg","outputId":"d262162d-4e88-4e17-a50f-8f86f1d200b5","executionInfo":{"status":"ok","timestamp":1747295696132,"user_tz":300,"elapsed":2160420,"user":{"displayName":"mk chung","userId":"05412849786518098631"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Fine-tuning Attention U-Net (SCSE, resnet50) ---\n","Using existing 'model_attention' instance.\n","Loading weights from /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_best_val_resnet50.pth for fine-tuning...\n","Successfully loaded pre-trained weights.\n","Initial GPU memory allocated for Fine-tuning Attention U-Net: 623.34 MB\n","Starting fine-tuning for 5 epochs with LR: 5e-05\n","Fine-tuning Epoch 1/5. Current LR: 5.00e-05\n","    Attention U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.2617\n","    Attention U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.3566\n","    Attention U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.2907\n","    Attention U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.2579\n","    Attention U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.1533\n","  Attention U-Net (Fine-tuning) - Epoch 1 Average Training Loss: 0.2790\n","  Attention U-Net (Fine-tuning) - Epoch 1 Average Validation Loss: 0.3563\n","    New best fine-tuned model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_ft_best_val_resnet50.pth (Val Loss: 0.3563)\n","Fine-tuning Epoch 2/5. Current LR: 5.00e-05\n","    Attention U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.1562\n","    Attention U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.2119\n","    Attention U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.1792\n","    Attention U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.2976\n","    Attention U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.2325\n","  Attention U-Net (Fine-tuning) - Epoch 2 Average Training Loss: 0.2644\n","  Attention U-Net (Fine-tuning) - Epoch 2 Average Validation Loss: 0.3490\n","    New best fine-tuned model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_ft_best_val_resnet50.pth (Val Loss: 0.3490)\n","Fine-tuning Epoch 3/5. Current LR: 5.00e-05\n","    Attention U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.3104\n","    Attention U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.2333\n","    Attention U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.2339\n","    Attention U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.2366\n","    Attention U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.3048\n","  Attention U-Net (Fine-tuning) - Epoch 3 Average Training Loss: 0.2582\n","  Attention U-Net (Fine-tuning) - Epoch 3 Average Validation Loss: 0.3473\n","    New best fine-tuned model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_ft_best_val_resnet50.pth (Val Loss: 0.3473)\n","Fine-tuning Epoch 4/5. Current LR: 5.00e-05\n","    Attention U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.1963\n","    Attention U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.2118\n","    Attention U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.4596\n","    Attention U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.0962\n","    Attention U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.3234\n","  Attention U-Net (Fine-tuning) - Epoch 4 Average Training Loss: 0.2534\n","  Attention U-Net (Fine-tuning) - Epoch 4 Average Validation Loss: 0.3453\n","    New best fine-tuned model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_ft_best_val_resnet50.pth (Val Loss: 0.3453)\n","Fine-tuning Epoch 5/5. Current LR: 5.00e-05\n","    Attention U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.1225\n","    Attention U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.2819\n","    Attention U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.5685\n","    Attention U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.2263\n","    Attention U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.2570\n","  Attention U-Net (Fine-tuning) - Epoch 5 Average Training Loss: 0.2491\n","  Attention U-Net (Fine-tuning) - Epoch 5 Average Validation Loss: 0.3525\n","--- Attention U-Net Fine-tuning Finished. Final model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_ft_final_epoch_resnet50.pth ---\n","Best fine-tuned validation model saved at /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/attention_unet_scse_ft_best_val_resnet50.pth with val_loss: 0.3453\n"]}],"source":["# --- Training Cell 9.2.1: Fine-tune Attention U-Net ---\n","print(f\"\\n--- Fine-tuning Attention U-Net (SCSE, {encoder_name}) ---\")\n","\n","# Define parameters for fine-tuning\n","num_ft_epochs = 5\n","fine_tune_lr = 5e-5\n","\n","# Paths for the fine-tuned model\n","model_name_attention_ft = \"attention_unet_scse_ft\"\n","best_model_attention_ft_path = os.path.join(save_dir, f\"{model_name_attention_ft}_best_val_resnet50.pth\")\n","final_model_attention_ft_path = os.path.join(save_dir, f\"{model_name_attention_ft}_final_epoch_resnet50.pth\")\n","\n","pretrained_model_path = os.path.join(save_dir, \"attention_unet_scse_best_val_resnet50.pth\")\n","\n","best_val_loss_attention_ft = 0.4018\n","\n","if not os.path.exists(pretrained_model_path):\n","    print(f\"Error: Pretrained model not found at {pretrained_model_path}. Cannot fine-tune.\")\n","    print(\"Please ensure the initial training (Cell 9.2) was run and the best model was saved correctly.\")\n","else:\n","    if train_loader and val_loader:\n","        if 'model_attention' not in globals() or model_attention is None: # Check if it needs redefinition\n","            print(\"Re-defining Attention U-Net model architecture for fine-tuning...\")\n","            model_attention = smp.Unet(\n","                encoder_name=encoder_name,\n","                encoder_weights=None,\n","                in_channels=3,\n","                classes=num_classes,\n","                activation=None,\n","                decoder_attention_type=\"scse\"\n","            )\n","            print(\"Attention U-Net model re-defined.\")\n","        else:\n","            print(\"Using existing 'model_attention' instance.\")\n","\n","\n","        print(f\"Loading weights from {pretrained_model_path} for fine-tuning...\")\n","        try:\n","            model_attention.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n","            print(\"Successfully loaded pre-trained weights.\")\n","        except Exception as e:\n","            print(f\"Error loading pre-trained weights: {e}\")\n","            raise\n","\n","        model_attention.to(device)\n","\n","        # Initialize a new optimizer with the fine-tuning learning rate\n","        optimizer_attention_ft = torch.optim.Adam(model_attention.parameters(), lr=fine_tune_lr, weight_decay=weight_decay)\n","\n","        # Initialize a new learning rate scheduler for fine-tuning\n","        scheduler_attention_ft = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_attention_ft, mode='min', factor=0.1, patience=3)\n","\n","        if device.type == \"cuda\":\n","            torch.cuda.empty_cache()\n","            print(f\"Initial GPU memory allocated for Fine-tuning Attention U-Net: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n","\n","        print(f\"Starting fine-tuning for {num_ft_epochs} epochs with LR: {fine_tune_lr}\")\n","        for epoch in range(num_ft_epochs):\n","            current_epoch_ft = epoch + 1 # For display\n","            print(f\"Fine-tuning Epoch {current_epoch_ft}/{num_ft_epochs}. Current LR: {optimizer_attention_ft.param_groups[0]['lr']:.2e}\") # Display current LR\n","\n","            # Training phase\n","            epoch_loss_attention_ft = train_one_epoch(model_attention, train_loader, optimizer_attention_ft, criterion, device, \"Attention U-Net (Fine-tuning)\")\n","            if epoch_loss_attention_ft == float('inf'):\n","                print(f\"Critical error fine-tuning Attention U-Net in epoch {current_epoch_ft}. Stopping fine-tuning.\")\n","                break\n","            print(f\"  Attention U-Net (Fine-tuning) - Epoch {current_epoch_ft} Average Training Loss: {epoch_loss_attention_ft:.4f}\")\n","\n","            # Validation phase\n","            current_val_loss_ft = evaluate_validation_loss(model_attention, val_loader, criterion, device, \"Attention U-Net (Fine-tuning Validation)\")\n","            print(f\"  Attention U-Net (Fine-tuning) - Epoch {current_epoch_ft} Average Validation Loss: {current_val_loss_ft:.4f}\")\n","\n","            if current_val_loss_ft < best_val_loss_attention_ft:\n","                best_val_loss_attention_ft = current_val_loss_ft\n","                torch.save(model_attention.state_dict(), best_model_attention_ft_path)\n","                print(f\"    New best fine-tuned model saved to {best_model_attention_ft_path} (Val Loss: {current_val_loss_ft:.4f})\")\n","\n","            # Step the learning rate scheduler\n","            scheduler_attention_ft.step(current_val_loss_ft)\n","\n","        # Save the final model state after fine-tuning\n","        torch.save(model_attention.state_dict(), final_model_attention_ft_path)\n","        print(f\"--- Attention U-Net Fine-tuning Finished. Final model saved to {final_model_attention_ft_path} ---\")\n","        if os.path.exists(best_model_attention_ft_path):\n","            print(f\"Best fine-tuned validation model saved at {best_model_attention_ft_path} with val_loss: {best_val_loss_attention_ft:.4f}\")\n","    else:\n","        if not train_loader: print(\"Skipping fine-tuning as train_loader is not available.\")\n","        if not val_loader: print(\"Skipping fine-tuning as val_loader is not available (validation is crucial for fine-tuning).\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1_aQzJ_fR-AcQcVYYLwxgZ5gvtN4h2M6a","timestamp":1747251036282}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2e58bdb6c19149b39685f37c91d3c94c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_39d0f1f795aa4744a87edf446d72e9c9","IPY_MODEL_7f1ded5990ad4ed4a1097798e8c08b82","IPY_MODEL_49d6b252e3d44883b587afaf8589f593"],"layout":"IPY_MODEL_9913c4c80e3a473794c6dc8d52d1b465"}},"39d0f1f795aa4744a87edf446d72e9c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb8a71616c6041f3960cd92d9a7c9ae8","placeholder":"​","style":"IPY_MODEL_61abbc89cc114cd49b238e4be25e144f","value":"config.json: 100%"}},"7f1ded5990ad4ed4a1097798e8c08b82":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8899cd68a83448b8ed1a5ea1852b55c","max":156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c2ce27dcaddd48e6ace7861d796b5b86","value":156}},"49d6b252e3d44883b587afaf8589f593":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b4eefc4d6d24652881b77cb240a92ed","placeholder":"​","style":"IPY_MODEL_27769240574d40eab11945bda3480170","value":" 156/156 [00:00&lt;00:00, 16.1kB/s]"}},"9913c4c80e3a473794c6dc8d52d1b465":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb8a71616c6041f3960cd92d9a7c9ae8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61abbc89cc114cd49b238e4be25e144f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8899cd68a83448b8ed1a5ea1852b55c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2ce27dcaddd48e6ace7861d796b5b86":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b4eefc4d6d24652881b77cb240a92ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27769240574d40eab11945bda3480170":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"baa9e8a9156648aab10d3b24bfac4e8a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dd577436993e4107a6f817d18a0b3693","IPY_MODEL_d135885ebecc4debbc8167ffac3e1fba","IPY_MODEL_35152a98c4a54085aef82285eb8e3d02"],"layout":"IPY_MODEL_86daa8cdfda24bf2ab14c9bd43dc60c7"}},"dd577436993e4107a6f817d18a0b3693":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be236d8207ec4d70b4fcc10aa5f2c5bf","placeholder":"​","style":"IPY_MODEL_c0a161c5d6ce4faaa3460515643ed231","value":"model.safetensors: 100%"}},"d135885ebecc4debbc8167ffac3e1fba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98112dd2b0db49ec8f1983f20a5b75f5","max":102464800,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81b4bbeab6fa48b29e517e330da1bdfa","value":102464800}},"35152a98c4a54085aef82285eb8e3d02":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97ebcf5389b641f3a8eb34cdf9f99c9a","placeholder":"​","style":"IPY_MODEL_2fc4db2529ac44d18496c01bf1618557","value":" 102M/102M [00:00&lt;00:00, 218MB/s]"}},"86daa8cdfda24bf2ab14c9bd43dc60c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be236d8207ec4d70b4fcc10aa5f2c5bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0a161c5d6ce4faaa3460515643ed231":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98112dd2b0db49ec8f1983f20a5b75f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81b4bbeab6fa48b29e517e330da1bdfa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"97ebcf5389b641f3a8eb34cdf9f99c9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fc4db2529ac44d18496c01bf1618557":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}