{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CAMAFfnpeUBv","outputId":"1cedb6b5-ab01-4144-f44d-28490a3aab91","executionInfo":{"status":"ok","timestamp":1747365136570,"user_tz":300,"elapsed":108353,"user":{"displayName":"mk chung","userId":"05412849786518098631"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Collecting segmentation-models-pytorch\n","  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.31.2)\n","Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.0.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.2.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\n","Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.15)\n","Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.4.26)\n","Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: segmentation-models-pytorch\n","Successfully installed segmentation-models-pytorch-0.5.0\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"]}],"source":["# CS444 U-Net Project Setup for Semantic Segmentation\n","# U-Net++ on Cityscapes dataset\n","\n","# Step 1: Install Required Libraries\n","!pip install torch torchvision\n","!pip install segmentation-models-pytorch\n","!pip install matplotlib # Ensure matplotlib is installed for visualization"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45494,"status":"ok","timestamp":1747366523588,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"P6ZJKRlgeaD3","outputId":"3de41911-54b1-4d93-fdba-4bdddea17e17"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Step 2: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":138,"status":"ok","timestamp":1747364820085,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"XIR_70UpEfCa","outputId":"3152d8f9-9205-4c8a-e384-ae1b58803b83"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34msaved_models\u001b[0m/\n"]}],"source":["ls '/content/drive/MyDrive/U-Net Segmentation Project/CityScapes'"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1020,"status":"ok","timestamp":1747364973961,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"IZZIn2y9ecdi","outputId":"1b53bece-e4e7-48b9-e67b-8e6b82a9cd18"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset structure verified: 'leftImg8bit' and 'gtFine' found in /content/drive/MyDrive/U-Net Segmentation Project/CityScapes.\n"]}],"source":["# Step 3: Verify Dataset Structure\n","import os\n","\n","# Update this path to your specific Google Drive location for the Cityscapes dataset\n","root_path = '/content/drive/MyDrive/U-Net Segmentation Project/CityScapes'\n","\n","if not (os.path.exists(os.path.join(root_path, 'leftImg8bit')) and os.path.exists(os.path.join(root_path, 'gtFine'))):\n","    raise RuntimeError(f\"Ensure 'leftImg8bit' and 'gtFine' folders are in the root directory: {root_path}. Please verify the path.\")\n","else:\n","    print(f\"Dataset structure verified: 'leftImg8bit' and 'gtFine' found in {root_path}.\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4211,"status":"ok","timestamp":1747364979826,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"_pz2bfSLefC2","outputId":"715759fb-268b-4da8-c2ae-55b1989b3dcb"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.6.0+cu124\n","CUDA available: True\n","CUDA version: 12.4\n","GPU device: Tesla T4\n"]}],"source":["# Step 4: Verify GPU Environment\n","import torch\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"CUDA version: {torch.version.cuda}\")\n","    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n","    device = torch.device(\"cuda\")\n","else:\n","    print(\"No GPU detected. Training will run on CPU, which will be very slow. Ensure GPU runtime is enabled in Colab (Runtime > Change runtime type).\")\n","    device = torch.device(\"cpu\")\n","\n","# Set environment variables for CUDA debugging (optional, can help diagnose issues)\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Synchronous CUDA errors\n","os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"    # Device-side assertions"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14312,"status":"ok","timestamp":1747364994437,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"qvwy07W_ehly","outputId":"5e1e7c1a-75f7-4846-916b-bbe6e840448f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded train dataset with 2975 samples.\n","Successfully loaded val dataset with 500 samples.\n","Sample image shape: torch.Size([3, 128, 256])\n","Sample target shape: torch.Size([128, 256])\n","Sample target unique values: tensor([  0,   1,   2,   5,   7,   8,   9,  10,  11,  13,  18, 255])\n"]}],"source":["# Step 5: Load Cityscapes Dataset\n","from torchvision.datasets import Cityscapes\n","from torchvision import transforms\n","import numpy as np\n","from PIL import Image # Needed for target resize resampling\n","\n","# Define transforms (resize to manage memory)\n","# Original image size for Cityscapes is 1024x2048. Resizing significantly.\n","img_height, img_width = 128, 256 # Reduced size for faster training and lower memory\n","\n","transform = transforms.Compose([\n","    transforms.Resize((img_height, img_width)),\n","    transforms.ToTensor()\n","])\n","\n","def target_to_tensor(target):\n","    # Resize target segmentation map. PIL Image object is expected by Resize.\n","    target = target.resize((img_width, img_height), resample=Image.NEAREST) # Nearest neighbor for labels\n","    target_np = np.array(target, dtype=np.uint8)\n","\n","    # Map Cityscapes labels to 0-18 (for 19 classes), set others to ignore_index (255)\n","    label_map = {\n","        # name: id, trainId\n","        'unlabeled': (0, 255), 'ego vehicle': (1, 255), 'rectification border': (2, 255),\n","        'out of roi': (3, 255), 'static': (4, 255), 'dynamic': (5, 255),\n","        'ground': (6, 255), 'road': (7, 0), 'sidewalk': (8, 1),\n","        'parking': (9, 255), 'rail track': (10, 255), 'building': (11, 2),\n","        'wall': (12, 3), 'fence': (13, 4), 'guard rail': (14, 255),\n","        'bridge': (15, 255), 'tunnel': (16, 255), 'pole': (17, 5),\n","        'polegroup': (18, 255), 'traffic light': (19, 6), 'traffic sign': (20, 7),\n","        'vegetation': (21, 8), 'terrain': (22, 9), 'sky': (23, 10),\n","        'person': (24, 11), 'rider': (25, 12), 'car': (26, 13),\n","        'truck': (27, 14), 'bus': (28, 15), 'caravan': (29, 255),\n","        'trailer': (30, 255), 'train': (31, 16), 'motorcycle': (32, 17),\n","        'bicycle': (33, 18)\n","    }\n","    # Create an empty array with ignore_index\n","    mapped_target = np.full_like(target_np, 255, dtype=np.uint8) # 255 for ignore_index\n","\n","    for cityscapes_id_tuple, train_id in label_map.items():\n","        original_id = train_id[0] # Using the 'id' from the tuple for mapping\n","        target_train_id = train_id[1]\n","        if target_train_id != 255: # If it's a valid trainId\n","            mapped_target[target_np == original_id] = target_train_id\n","\n","    return torch.from_numpy(mapped_target).long()\n","\n","# Load training and validation datasets\n","try:\n","    train_dataset = Cityscapes(\n","        root=root_path,\n","        split='train',\n","        mode='fine',\n","        target_type='semantic',\n","        transform=transform,\n","        target_transform=target_to_tensor\n","    )\n","    val_dataset = Cityscapes(\n","        root=root_path,\n","        split='val',\n","        mode='fine',\n","        target_type='semantic',\n","        transform=transform,\n","        target_transform=target_to_tensor\n","    )\n","    print(f\"Successfully loaded train dataset with {len(train_dataset)} samples.\")\n","    print(f\"Successfully loaded val dataset with {len(val_dataset)} samples.\")\n","\n","except Exception as e:\n","    print(f\"Error loading dataset: {e}\")\n","    print(\"Please ensure your `root_path` is correct and the dataset is properly structured.\")\n","    raise\n","\n","# Verify dataset labels from a sample\n","if len(train_dataset) > 0:\n","    sample_image, sample_target = train_dataset[0]\n","    print(f\"Sample image shape: {sample_image.shape}\")\n","    print(f\"Sample target shape: {sample_target.shape}\")\n","    unique_labels = torch.unique(sample_target)\n","    print(f\"Sample target unique values: {unique_labels}\")\n","    if not (all( (unique_labels >= 0) & (unique_labels <= 18) | (unique_labels == 255) )):\n","        print(\"Warning: Unexpected label values found in sample target.\")\n","else:\n","    print(\"Train dataset is empty. Cannot verify sample.\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11772,"status":"ok","timestamp":1747365006211,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"wcmRHoRUellz","outputId":"98c99be7-ea7f-4f95-b206-23b1cd822287"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking train dataset integrity (first 5 samples)...\n","train dataset integrity verified for first 5 samples.\n","Checking val dataset integrity (first 5 samples)...\n","val dataset integrity verified for first 5 samples.\n"]}],"source":["# Step 5.1: Verify Dataset Integrity\n","def check_dataset_integrity(dataset, split_name, num_samples_to_check=5):\n","    print(f\"Checking {split_name} dataset integrity (first {num_samples_to_check} samples)...\")\n","    if len(dataset) == 0:\n","        print(f\"{split_name} dataset is empty. Skipping integrity check.\")\n","        return True\n","    for i in range(min(num_samples_to_check, len(dataset))):\n","        try:\n","            image, target = dataset[i]\n","            # Basic checks\n","            if not isinstance(image, torch.Tensor) or not isinstance(target, torch.Tensor):\n","                print(f\"Error at index {i}: Image or target is not a tensor.\")\n","                return False\n","            if image.shape != torch.Size([3, img_height, img_width]):\n","                 print(f\"Error at index {i}: Unexpected image shape {image.shape}.\")\n","                 return False\n","            if target.shape != torch.Size([img_height, img_width]):\n","                 print(f\"Error at index {i}: Unexpected target shape {target.shape}.\")\n","                 return False\n","            if not ((target >= 0) & (target <= 18) | (target == 255)).all():\n","                 print(f\"Error at index {i}: Target contains invalid labels {torch.unique(target)}.\")\n","                 return False\n","\n","        except Exception as e:\n","            print(f\"Error accessing sample {i} in {split_name} dataset: {e}\")\n","            return False\n","    print(f\"{split_name} dataset integrity verified for first {num_samples_to_check} samples.\")\n","    return True\n","\n","# Check train and validation datasets (first few samples)\n","if not check_dataset_integrity(train_dataset, \"train\"):\n","    raise RuntimeError(\"Train dataset integrity check failed.\")\n","if not check_dataset_integrity(val_dataset, \"val\"):\n","    raise RuntimeError(\"Validation dataset integrity check failed.\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1747365019617,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"4wQ7hLvpeogt","outputId":"09e1c827-cf90-45f7-be7e-f9ea9f589afb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train loader created with batch size 2.\n","Validation loader created with batch size 2.\n"]}],"source":["# Step 6: Create Data Loaders\n","from torch.utils.data import DataLoader\n","\n","batch_size = 2\n","\n","if len(train_dataset) > 0:\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","else:\n","    train_loader = None\n","    print(\"Train dataset is empty. Train loader not created.\")\n","\n","if len(val_dataset) > 0:\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","else:\n","    val_loader = None\n","    print(\"Validation dataset is empty. Validation loader not created.\")\n","\n","if train_loader:\n","    print(f\"Train loader created with batch size {batch_size}.\")\n","if val_loader:\n","    print(f\"Validation loader created with batch size {batch_size}.\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223,"referenced_widgets":["472e26a64ce140c18200ecc28a1335ee","26afe3290ed74d7bbb8140fcb0efecbc","271ba5ec27704e16a199fe3e0b8ad2c2","ed9109c1db614cf8b28f65a9ba5a02bd","fcec9b5fbf9f4848a611c34c41afa01b","c5f3704027434b04923468e6a3a5858e","93284d1057e74a69aa11e9f4ab4f0c27","b14c566520bc4225b97d6699a7024517","79fe56a4dcbc4a4abd57b074c90052e0","72e5432fddae49d4adece14a26510beb","e85fb3e63a3445d285c4cf3328fba9c5","e260d5e50f9f472eafe0b55033d986a6","a8e985fffd40446fa16bb57838aa8113","cf4e9d8e5976401dac04ae3907ca0c8b","bf10bcbe72a1416dbf090ef27dad6d1a","8530a07f179649d987b58d786bd4b04a","a40b9c8848284a9d95ac44d00a6dbdef","e0ae8d0a4b4e42b5833728197892d9f4","696d5e6551d741e4972ec648402bff71","bff0ea9d8e3e49149b2eeb91f5483de4","525c9656245d4a09a297f55f02fd0d97","b9ce92aefb984ff8b3c5bbbf70163e65"]},"executionInfo":{"elapsed":11112,"status":"ok","timestamp":1747365147714,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"E07uINSEerSA","outputId":"7d58339e-dad6-44a2-af28-dfc89df790ea"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/106 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"472e26a64ce140c18200ecc28a1335ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e260d5e50f9f472eafe0b55033d986a6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["U-Net++ with efficientnet-b3 encoder defined.\n"]}],"source":["# Step 7: Define Models\n","import segmentation_models_pytorch as smp\n","\n","num_classes = 19  # Based on the label mapping (0-18)\n","encoder_name = \"efficientnet-b3\"\n","\n","# U-Net++\n","model_unetpp = smp.UnetPlusPlus(\n","    encoder_name=encoder_name,\n","    encoder_weights=\"imagenet\",\n","    in_channels=3,\n","    classes=num_classes,\n","    activation=None\n",")\n","print(f\"U-Net++ with {encoder_name} encoder defined.\")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1747365147737,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"UsTMZjp8eubU","outputId":"c848867f-943c-4ca7-fe7e-bf74d8117bfd"},"outputs":[{"output_type":"stream","name":"stdout","text":["CrossEntropyLoss defined with ignore_index=255.\n"]}],"source":["# Step 8: Define Loss Function\n","import torch.nn as nn\n","# CrossEntropyLoss expects raw logits from the model and long type targets.\n","# ignore_index=255 means that pixels with label 255 will not contribute to the loss.\n","criterion = nn.CrossEntropyLoss(ignore_index=255)\n","print(\"CrossEntropyLoss defined with ignore_index=255.\")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1747365147753,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"tn_l-Gsvg6El","outputId":"775ecda6-103a-44bb-90f1-1e37a61aebf9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Models will be saved in: /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models\n"]}],"source":["# Step 8.1: Setup Model Saving and Validation Loss Function\n","import os\n","\n","save_dir = os.path.join(root_path, 'saved_models')\n","os.makedirs(save_dir, exist_ok=True)\n","print(f\"Models will be saved in: {save_dir}\")\n","\n","def evaluate_validation_loss(model, loader, criterion, device, model_name):\n","    model.eval() # Set model to evaluation mode\n","    running_val_loss = 0.0\n","    num_batches = len(loader)\n","\n","    if num_batches == 0:\n","        print(f\"Warning: Validation loader for {model_name} is empty. Cannot compute validation loss.\")\n","        return float('inf') # Return infinity if no validation can be done\n","\n","    with torch.no_grad(): # No gradients needed for validation\n","        for images, targets in loader:\n","            images, targets = images.to(device), targets.to(device)\n","\n","            # Ensure targets are valid before calculating loss\n","            if not ((targets >= 0) & (targets < num_classes) | (targets == 255)).all():\n","                print(f\"Validation: Invalid labels detected in targets for {model_name}! Unique: {torch.unique(targets)}. Skipping batch for loss calculation.\")\n","                pass\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, targets)\n","            running_val_loss += loss.item()\n","\n","    avg_val_loss = running_val_loss / num_batches if num_batches > 0 else float('inf')\n","    if device.type == \"cuda\":\n","        torch.cuda.empty_cache()\n","    return avg_val_loss"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"JsHCbdHPexJ-","executionInfo":{"status":"ok","timestamp":1747365147863,"user_tz":300,"elapsed":86,"user":{"displayName":"mk chung","userId":"05412849786518098631"}}},"outputs":[],"source":["# Step 9: Training Configuration\n","num_epochs = 15\n","learning_rate = 1e-5\n","weight_decay = 1e-5\n","\n","# Helper function for training one epoch\n","def train_one_epoch(model, loader, optimizer, criterion, device, model_name):\n","    model.train()\n","    running_loss = 0.0\n","    num_batches = len(loader)\n","\n","    if num_batches == 0:\n","        print(f\"Warning: Training loader for {model_name} is empty. Skipping training epoch.\")\n","        return 0.0\n","\n","    for i, (images, targets) in enumerate(loader):\n","        images, targets = images.to(device), targets.to(device)\n","\n","        try:\n","            # Verify target labels before passing to model/loss\n","            if not ((targets >= 0) & (targets < num_classes) | (targets == 255)).all():\n","                print(f\"Batch {i}: Invalid labels detected in targets for {model_name}! Unique: {torch.unique(targets)}. Skipping batch.\")\n","                problematic_targets = targets.clone()\n","                problematic_targets[(targets >= 0) & (targets < num_classes) | (targets == 255)] = -1 # Mark valid ones\n","                print(f\"Problematic target values: {torch.unique(problematic_targets.masked_select(problematic_targets != -1))}\")\n","                continue # Skip this batch\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","            if (i + 1) % (num_batches // 5 if num_batches >= 5 else 1) == 0: # Print 5 times per epoch\n","                print(f\"    {model_name} - Batch {i+1}/{num_batches}, Loss: {loss.item():.4f}\")\n","\n","        except RuntimeError as e:\n","            print(f\"RuntimeError during training {model_name} at batch {i}: {e}\")\n","            if \"CUDA out of memory\" in str(e):\n","                print(\"CUDA OOM: Try reducing batch size or image dimensions.\")\n","                if device.type == \"cuda\": torch.cuda.empty_cache()\n","            return float('inf') # Indicate critical error\n","\n","    epoch_loss = running_loss / num_batches if num_batches > 0 else 0\n","    if device.type == \"cuda\":\n","        torch.cuda.empty_cache()\n","    return epoch_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Afso8WQke4Sz","outputId":"424ce445-7df1-4cd9-92e0-be5be4ca4a47"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Training U-Net++ (efficientnet-b3) ---\n","Initial GPU memory allocated for U-Net++: 266.00 MB\n","Epoch 1/15\n","    U-Net++ - Batch 297/1488, Loss: 2.3609\n","    U-Net++ - Batch 594/1488, Loss: 1.8789\n","    U-Net++ - Batch 891/1488, Loss: 1.5597\n","    U-Net++ - Batch 1188/1488, Loss: 1.4397\n","    U-Net++ - Batch 1485/1488, Loss: 1.4296\n","  U-Net++ - Epoch 1 Average Training Loss: 1.9196\n","  U-Net++ - Epoch 1 Average Validation Loss: 1.2106\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 1.2106)\n","Epoch 2/15\n","    U-Net++ - Batch 297/1488, Loss: 1.0530\n","    U-Net++ - Batch 594/1488, Loss: 1.0026\n","    U-Net++ - Batch 891/1488, Loss: 0.7783\n","    U-Net++ - Batch 1188/1488, Loss: 0.9859\n","    U-Net++ - Batch 1485/1488, Loss: 0.7971\n","  U-Net++ - Epoch 2 Average Training Loss: 1.0144\n","  U-Net++ - Epoch 2 Average Validation Loss: 0.8086\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 0.8086)\n","Epoch 3/15\n","    U-Net++ - Batch 297/1488, Loss: 0.5591\n","    U-Net++ - Batch 594/1488, Loss: 1.0489\n","    U-Net++ - Batch 891/1488, Loss: 0.5291\n","    U-Net++ - Batch 1188/1488, Loss: 0.7471\n","    U-Net++ - Batch 1485/1488, Loss: 0.5694\n","  U-Net++ - Epoch 3 Average Training Loss: 0.7705\n","  U-Net++ - Epoch 3 Average Validation Loss: 0.6575\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 0.6575)\n","Epoch 4/15\n","    U-Net++ - Batch 297/1488, Loss: 0.8363\n","    U-Net++ - Batch 594/1488, Loss: 0.8540\n","    U-Net++ - Batch 891/1488, Loss: 0.6422\n","    U-Net++ - Batch 1188/1488, Loss: 0.8622\n","    U-Net++ - Batch 1485/1488, Loss: 0.4882\n","  U-Net++ - Epoch 4 Average Training Loss: 0.6653\n","  U-Net++ - Epoch 4 Average Validation Loss: 0.5995\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 0.5995)\n","Epoch 5/15\n","    U-Net++ - Batch 297/1488, Loss: 0.9034\n","    U-Net++ - Batch 594/1488, Loss: 0.7423\n","    U-Net++ - Batch 891/1488, Loss: 0.7025\n","    U-Net++ - Batch 1188/1488, Loss: 0.4662\n","    U-Net++ - Batch 1485/1488, Loss: 0.3937\n","  U-Net++ - Epoch 5 Average Training Loss: 0.5990\n","  U-Net++ - Epoch 5 Average Validation Loss: 0.5671\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 0.5671)\n","Epoch 6/15\n","    U-Net++ - Batch 297/1488, Loss: 0.6311\n","    U-Net++ - Batch 594/1488, Loss: 0.9939\n","    U-Net++ - Batch 891/1488, Loss: 0.8126\n","    U-Net++ - Batch 1188/1488, Loss: 0.4762\n","    U-Net++ - Batch 1485/1488, Loss: 1.0888\n","  U-Net++ - Epoch 6 Average Training Loss: 0.5575\n","  U-Net++ - Epoch 6 Average Validation Loss: 0.5455\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 0.5455)\n","Epoch 7/15\n","    U-Net++ - Batch 297/1488, Loss: 0.7875\n","    U-Net++ - Batch 594/1488, Loss: 0.8493\n","    U-Net++ - Batch 891/1488, Loss: 0.5561\n","    U-Net++ - Batch 1188/1488, Loss: 0.4383\n","    U-Net++ - Batch 1485/1488, Loss: 0.5148\n","  U-Net++ - Epoch 7 Average Training Loss: 0.5258\n","  U-Net++ - Epoch 7 Average Validation Loss: 0.5260\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 0.5260)\n","Epoch 8/15\n","    U-Net++ - Batch 297/1488, Loss: 0.4260\n","    U-Net++ - Batch 594/1488, Loss: 0.3260\n","    U-Net++ - Batch 891/1488, Loss: 0.4494\n","    U-Net++ - Batch 1188/1488, Loss: 0.4041\n","    U-Net++ - Batch 1485/1488, Loss: 0.5905\n","  U-Net++ - Epoch 8 Average Training Loss: 0.5001\n","  U-Net++ - Epoch 8 Average Validation Loss: 0.5140\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 0.5140)\n","Epoch 9/15\n","    U-Net++ - Batch 297/1488, Loss: 0.5067\n","    U-Net++ - Batch 594/1488, Loss: 0.4801\n","    U-Net++ - Batch 891/1488, Loss: 0.3182\n","    U-Net++ - Batch 1188/1488, Loss: 0.3101\n","    U-Net++ - Batch 1485/1488, Loss: 0.3313\n","  U-Net++ - Epoch 9 Average Training Loss: 0.4797\n","  U-Net++ - Epoch 9 Average Validation Loss: 0.5046\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 0.5046)\n","Epoch 10/15\n","    U-Net++ - Batch 297/1488, Loss: 0.5276\n","    U-Net++ - Batch 594/1488, Loss: 0.4026\n","    U-Net++ - Batch 891/1488, Loss: 0.4497\n","    U-Net++ - Batch 1188/1488, Loss: 0.3430\n","    U-Net++ - Batch 1485/1488, Loss: 0.8765\n","  U-Net++ - Epoch 10 Average Training Loss: 0.4597\n","  U-Net++ - Epoch 10 Average Validation Loss: 0.4969\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 0.4969)\n","Epoch 11/15\n","    U-Net++ - Batch 297/1488, Loss: 0.4131\n","    U-Net++ - Batch 594/1488, Loss: 0.3361\n","    U-Net++ - Batch 891/1488, Loss: 0.3446\n","    U-Net++ - Batch 1188/1488, Loss: 0.2617\n","    U-Net++ - Batch 1485/1488, Loss: 0.4017\n","  U-Net++ - Epoch 11 Average Training Loss: 0.4441\n","  U-Net++ - Epoch 11 Average Validation Loss: 0.4856\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 0.4856)\n","Epoch 12/15\n","    U-Net++ - Batch 297/1488, Loss: 0.2452\n","    U-Net++ - Batch 594/1488, Loss: 0.3803\n","    U-Net++ - Batch 891/1488, Loss: 0.4066\n","    U-Net++ - Batch 1188/1488, Loss: 0.5359\n","    U-Net++ - Batch 1485/1488, Loss: 0.5844\n","  U-Net++ - Epoch 12 Average Training Loss: 0.4300\n","  U-Net++ - Epoch 12 Average Validation Loss: 0.4825\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 0.4825)\n","Epoch 13/15\n","    U-Net++ - Batch 297/1488, Loss: 0.4027\n","    U-Net++ - Batch 594/1488, Loss: 0.4593\n","    U-Net++ - Batch 891/1488, Loss: 0.1898\n","    U-Net++ - Batch 1188/1488, Loss: 0.2881\n","    U-Net++ - Batch 1485/1488, Loss: 0.3094\n","  U-Net++ - Epoch 13 Average Training Loss: 0.4187\n","  U-Net++ - Epoch 13 Average Validation Loss: 0.4740\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 0.4740)\n","Epoch 14/15\n","    U-Net++ - Batch 297/1488, Loss: 0.4659\n","    U-Net++ - Batch 594/1488, Loss: 0.2871\n","    U-Net++ - Batch 891/1488, Loss: 0.3378\n","    U-Net++ - Batch 1188/1488, Loss: 0.4403\n","    U-Net++ - Batch 1485/1488, Loss: 0.4328\n","  U-Net++ - Epoch 14 Average Training Loss: 0.4064\n","  U-Net++ - Epoch 14 Average Validation Loss: 0.4708\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 0.4708)\n","Epoch 15/15\n","    U-Net++ - Batch 297/1488, Loss: 0.5509\n","    U-Net++ - Batch 594/1488, Loss: 0.2497\n","    U-Net++ - Batch 891/1488, Loss: 0.3613\n","    U-Net++ - Batch 1188/1488, Loss: 0.3307\n","    U-Net++ - Batch 1485/1488, Loss: 0.7594\n","  U-Net++ - Epoch 15 Average Training Loss: 0.3960\n","  U-Net++ - Epoch 15 Average Validation Loss: 0.4655\n","    New best model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth (Val Loss: 0.4655)\n","--- U-Net++ Training Finished. Final model saved to /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_final_epoch.pth ---\n","Best validation model saved at /content/drive/MyDrive/ECE 494 Group Project/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth with val_loss: 0.4655\n"]}],"source":["# --- Training Cell 9.3: U-Net++ ---\n","print(f\"\\n--- Training U-Net++ ({encoder_name}) ---\")\n","model_name_unetpp = \"unetplusplus\"\n","best_model_unetpp_path = os.path.join(save_dir, f\"{model_name_unetpp}_best_val.pth\")\n","final_model_unetpp_path = os.path.join(save_dir, f\"{model_name_unetpp}_final_epoch.pth\")\n","best_val_loss_unetpp = float('inf')\n","\n","if train_loader:\n","    model_unetpp.to(device)\n","    optimizer_unetpp = torch.optim.Adam(model_unetpp.parameters(), lr=learning_rate)\n","    sheduler_unetpp = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_unetpp, mode='min', patience=3, factor=0.5)\n","\n","    if device.type == \"cuda\":\n","        torch.cuda.empty_cache()\n","        print(f\"Initial GPU memory allocated for U-Net++: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        # Training phase\n","        epoch_loss_unetpp = train_one_epoch(model_unetpp, train_loader, optimizer_unetpp, criterion, device, \"U-Net++\")\n","        if epoch_loss_unetpp == float('inf'):\n","            print(f\"Critical error training U-Net++ in epoch {epoch+1}. Stopping training for this model.\")\n","            break\n","        print(f\"  U-Net++ - Epoch {epoch+1} Average Training Loss: {epoch_loss_unetpp:.4f}\")\n","\n","        # Validation phase\n","        if val_loader:\n","            current_val_loss = evaluate_validation_loss(model_unetpp, val_loader, criterion, device, \"U-Net++ (Validation)\")\n","            print(f\"  U-Net++ - Epoch {epoch+1} Average Validation Loss: {current_val_loss:.4f}\")\n","\n","            sheduler_unetpp.step(current_val_loss)\n","\n","            if current_val_loss < best_val_loss_unetpp:\n","                best_val_loss_unetpp = current_val_loss\n","                torch.save(model_unetpp.state_dict(), best_model_unetpp_path)\n","                print(f\"    New best model saved to {best_model_unetpp_path} (Val Loss: {current_val_loss:.4f})\")\n","        else:\n","            print(\"  Skipping validation for checkpointing as val_loader is not available.\")\n","\n","    # Save the final model state\n","    torch.save(model_unetpp.state_dict(), final_model_unetpp_path)\n","    print(f\"--- U-Net++ Training Finished. Final model saved to {final_model_unetpp_path} ---\")\n","    if val_loader and os.path.exists(best_model_unetpp_path):\n","        print(f\"Best validation model saved at {best_model_unetpp_path} with val_loss: {best_val_loss_unetpp:.4f}\")\n","else:\n","    print(\"Skipping U-Net++ training as train_loader is not available.\")"]},{"cell_type":"code","source":["print(f\"\\n--- Fine-tuning U-Net++ ({encoder_name}) ---\")\n","\n","num_ft_epochs = 5\n","fine_tune_lr = 5e-5\n","\n","model_name_unetpp_ft = \"unetplusplus_ft\"\n","best_model_unetpp_ft_path = os.path.join(save_dir, f\"{model_name_unetpp_ft}_best_val.pth\")\n","final_model_unetpp_ft_path = os.path.join(save_dir, f\"{model_name_unetpp_ft}_final_epoch.pth\")\n","\n","pretrained_model_unetpp_path = os.path.join(save_dir, \"unetplusplus_best_val.pth\")\n","\n","best_val_loss_unetpp_ft = best_val_loss_unetpp if 'best_val_loss_unetpp' in globals() else float('inf')\n","\n","if not os.path.exists(pretrained_model_unetpp_path):\n","    print(f\"Error: Pretrained model not found at {pretrained_model_unetpp_path}. Cannot fine-tune.\")\n","else:\n","    if train_loader and val_loader:\n","\n","        if 'model_unetpp' not in globals() or model_unetpp is None:\n","            print(\"Re-defining U-Net++ model architecture for fine-tuning...\")\n","            model_unetpp = smp.UnetPlusPlus(\n","                encoder_name=encoder_name,\n","                encoder_weights=None,\n","                in_channels=3,\n","                classes=num_classes,\n","                activation=None\n","            )\n","            print(\"U-Net++ model re-defined.\")\n","        else:\n","            print(\"Using existing 'model_unetpp' instance.\")\n","\n","        print(f\"Loading weights from {pretrained_model_unetpp_path} for fine-tuning...\")\n","        try:\n","            model_unetpp.load_state_dict(torch.load(pretrained_model_unetpp_path, map_location=device))\n","            print(\"Successfully loaded pre-trained weights.\")\n","        except Exception as e:\n","            print(f\"Error loading weights: {e}\")\n","            raise\n","\n","        model_unetpp.to(device)\n","        optimizer_unetpp_ft = torch.optim.Adam(model_unetpp.parameters(), lr=fine_tune_lr, weight_decay=weight_decay)\n","        scheduler_unetpp_ft = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_unetpp_ft, mode='min', factor=0.1, patience=3)\n","\n","        if device.type == \"cuda\":\n","            torch.cuda.empty_cache()\n","            print(f\"Initial GPU memory allocated for Fine-tuning U-Net++: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n","\n","        print(f\"Starting fine-tuning for {num_ft_epochs} epochs with LR: {fine_tune_lr}\")\n","        for epoch in range(num_ft_epochs):\n","            current_epoch_ft = epoch + 1\n","            print(f\"Fine-tuning Epoch {current_epoch_ft}/{num_ft_epochs}. Current LR: {optimizer_unetpp_ft.param_groups[0]['lr']:.2e}\")\n","\n","            # Training phase\n","            epoch_loss_unetpp_ft = train_one_epoch(model_unetpp, train_loader, optimizer_unetpp_ft, criterion, device, \"U-Net++ (Fine-tuning)\")\n","            if epoch_loss_unetpp_ft == float('inf'):\n","                print(f\"Critical error fine-tuning U-Net++ in epoch {current_epoch_ft}. Stopping.\")\n","                break\n","            print(f\"  U-Net++ (Fine-tuning) - Epoch {current_epoch_ft} Avg Training Loss: {epoch_loss_unetpp_ft:.4f}\")\n","\n","            # Validation phase\n","            current_val_loss_ft = evaluate_validation_loss(model_unetpp, val_loader, criterion, device, \"U-Net++ (Fine-tuning Validation)\")\n","            print(f\"  U-Net++ (Fine-tuning) - Epoch {current_epoch_ft} Avg Validation Loss: {current_val_loss_ft:.4f}\")\n","\n","            if current_val_loss_ft < best_val_loss_unetpp_ft:\n","                best_val_loss_unetpp_ft = current_val_loss_ft\n","                torch.save(model_unetpp.state_dict(), best_model_unetpp_ft_path)\n","                print(f\"    New best fine-tuned model saved to {best_model_unetpp_ft_path} (Val Loss: {current_val_loss_ft:.4f})\")\n","\n","            scheduler_unetpp_ft.step(current_val_loss_ft)\n","\n","        # Save final model after all fine-tuning epochs\n","        torch.save(model_unetpp.state_dict(), final_model_unetpp_ft_path)\n","        print(f\"--- U-Net++ Fine-tuning Finished. Final model saved to {final_model_unetpp_ft_path} ---\")\n","        if os.path.exists(best_model_unetpp_ft_path):\n","            print(f\"Best fine-tuned validation model saved at {best_model_unetpp_ft_path} with val_loss: {best_val_loss_unetpp_ft:.4f}\")\n","    else:\n","        if not train_loader: print(\"Skipping fine-tuning as train_loader is not available.\")\n","        if not val_loader: print(\"Skipping fine-tuning as val_loader is not available.\")\n"],"metadata":{"id":"xDxOKgibc1HS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747367070909,"user_tz":300,"elapsed":41,"user":{"displayName":"mk chung","userId":"05412849786518098631"}},"outputId":"1648f83e-b295-4aed-a5d6-947cc96a3113"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Fine-tuning U-Net++ (resnet34) ---\n","Using existing 'model_unetpp' instance.\n","Loading weights from /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_best_val.pth for fine-tuning...\n","Successfully loaded pre-trained weights.\n","Initial GPU memory allocated for Fine-tuning U-Net++: 100.81 MB\n","Starting fine-tuning for 5 epochs with LR: 5e-05\n","Fine-tuning Epoch 1/5. Current LR: 5.00e-05\n","    U-Net++ (Fine-tuning) - Batch 297/1488, Loss: 0.1300\n","    U-Net++ (Fine-tuning) - Batch 594/1488, Loss: 0.1536\n","    U-Net++ (Fine-tuning) - Batch 891/1488, Loss: 0.2295\n","    U-Net++ (Fine-tuning) - Batch 1188/1488, Loss: 0.1687\n","    U-Net++ (Fine-tuning) - Batch 1485/1488, Loss: 0.1420\n","  U-Net++ (Fine-tuning) - Epoch 1 Avg Training Loss: 0.1797\n","  U-Net++ (Fine-tuning) - Epoch 1 Avg Validation Loss: 0.3380\n","    New best fine-tuned model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_ft_best_val.pth (Val Loss: 0.3380)\n","Fine-tuning Epoch 2/5. Current LR: 5.00e-05\n","    U-Net++ (Fine-tuning) - Batch 297/1488, Loss: 0.1693\n","    U-Net++ (Fine-tuning) - Batch 594/1488, Loss: 0.1729\n","    U-Net++ (Fine-tuning) - Batch 891/1488, Loss: 0.1564\n","    U-Net++ (Fine-tuning) - Batch 1188/1488, Loss: 0.1304\n","    U-Net++ (Fine-tuning) - Batch 1485/1488, Loss: 0.1067\n","  U-Net++ (Fine-tuning) - Epoch 2 Avg Training Loss: 0.1662\n","  U-Net++ (Fine-tuning) - Epoch 2 Avg Validation Loss: 0.3538\n","Fine-tuning Epoch 3/5. Current LR: 5.00e-05\n","    U-Net++ (Fine-tuning) - Batch 297/1488, Loss: 0.1654\n","    U-Net++ (Fine-tuning) - Batch 594/1488, Loss: 0.1078\n","    U-Net++ (Fine-tuning) - Batch 891/1488, Loss: 0.1882\n","    U-Net++ (Fine-tuning) - Batch 1188/1488, Loss: 0.0896\n","    U-Net++ (Fine-tuning) - Batch 1485/1488, Loss: 0.1071\n","  U-Net++ (Fine-tuning) - Epoch 3 Avg Training Loss: 0.1578\n","  U-Net++ (Fine-tuning) - Epoch 3 Avg Validation Loss: 0.3482\n","Fine-tuning Epoch 4/5. Current LR: 5.00e-05\n","    U-Net++ (Fine-tuning) - Batch 297/1488, Loss: 0.1307\n","    U-Net++ (Fine-tuning) - Batch 594/1488, Loss: 0.2294\n","    U-Net++ (Fine-tuning) - Batch 891/1488, Loss: 0.1829\n","    U-Net++ (Fine-tuning) - Batch 1188/1488, Loss: 0.1205\n","    U-Net++ (Fine-tuning) - Batch 1485/1488, Loss: 0.1435\n","  U-Net++ (Fine-tuning) - Epoch 3 Avg Training Loss: 0.1877\n","  U-Net++ (Fine-tuning) - Epoch 3 Avg Validation Loss: 0.3613\n","Fine-tuning Epoch 5/5. Current LR: 5.00e-05\n","    U-Net++ (Fine-tuning) - Batch 297/1488, Loss: 0.1671\n","    U-Net++ (Fine-tuning) - Batch 594/1488, Loss: 0.2021\n","    U-Net++ (Fine-tuning) - Batch 891/1488, Loss: 0.2031\n","    U-Net++ (Fine-tuning) - Batch 1188/1488, Loss: 0.1403\n","    U-Net++ (Fine-tuning) - Batch 1485/1488, Loss: 0.1980\n","  U-Net++ (Fine-tuning) - Epoch 5 Average Training Loss: 0.1895\n","  U-Net++ (Fine-tuning) - Epoch 5 Average Validation Loss: 0.3594\n","--- U-Net++ Fine-tuning Finished. Final model saved to /content/drive/MyDrive/Colab Notebooks/CS444/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_ft_final_epoch.pth ---\n","Best fine-tuned validation model saved at /content/drive/MyDrive/Colab Notebooks/CS444/U-Net Segmentation Project/CityScapes/saved_models/unetplusplus_ft_best_val.pth with val_loss: 0.3380\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"3K3qC-v3DeeL"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"472e26a64ce140c18200ecc28a1335ee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26afe3290ed74d7bbb8140fcb0efecbc","IPY_MODEL_271ba5ec27704e16a199fe3e0b8ad2c2","IPY_MODEL_ed9109c1db614cf8b28f65a9ba5a02bd"],"layout":"IPY_MODEL_fcec9b5fbf9f4848a611c34c41afa01b"}},"26afe3290ed74d7bbb8140fcb0efecbc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5f3704027434b04923468e6a3a5858e","placeholder":"​","style":"IPY_MODEL_93284d1057e74a69aa11e9f4ab4f0c27","value":"config.json: 100%"}},"271ba5ec27704e16a199fe3e0b8ad2c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b14c566520bc4225b97d6699a7024517","max":106,"min":0,"orientation":"horizontal","style":"IPY_MODEL_79fe56a4dcbc4a4abd57b074c90052e0","value":106}},"ed9109c1db614cf8b28f65a9ba5a02bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72e5432fddae49d4adece14a26510beb","placeholder":"​","style":"IPY_MODEL_e85fb3e63a3445d285c4cf3328fba9c5","value":" 106/106 [00:00&lt;00:00, 2.39kB/s]"}},"fcec9b5fbf9f4848a611c34c41afa01b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5f3704027434b04923468e6a3a5858e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93284d1057e74a69aa11e9f4ab4f0c27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b14c566520bc4225b97d6699a7024517":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79fe56a4dcbc4a4abd57b074c90052e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"72e5432fddae49d4adece14a26510beb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e85fb3e63a3445d285c4cf3328fba9c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e260d5e50f9f472eafe0b55033d986a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a8e985fffd40446fa16bb57838aa8113","IPY_MODEL_cf4e9d8e5976401dac04ae3907ca0c8b","IPY_MODEL_bf10bcbe72a1416dbf090ef27dad6d1a"],"layout":"IPY_MODEL_8530a07f179649d987b58d786bd4b04a"}},"a8e985fffd40446fa16bb57838aa8113":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a40b9c8848284a9d95ac44d00a6dbdef","placeholder":"​","style":"IPY_MODEL_e0ae8d0a4b4e42b5833728197892d9f4","value":"model.safetensors: 100%"}},"cf4e9d8e5976401dac04ae3907ca0c8b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_696d5e6551d741e4972ec648402bff71","max":49336200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bff0ea9d8e3e49149b2eeb91f5483de4","value":49336200}},"bf10bcbe72a1416dbf090ef27dad6d1a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_525c9656245d4a09a297f55f02fd0d97","placeholder":"​","style":"IPY_MODEL_b9ce92aefb984ff8b3c5bbbf70163e65","value":" 49.3M/49.3M [00:00&lt;00:00, 123MB/s]"}},"8530a07f179649d987b58d786bd4b04a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a40b9c8848284a9d95ac44d00a6dbdef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0ae8d0a4b4e42b5833728197892d9f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"696d5e6551d741e4972ec648402bff71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bff0ea9d8e3e49149b2eeb91f5483de4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"525c9656245d4a09a297f55f02fd0d97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9ce92aefb984ff8b3c5bbbf70163e65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}