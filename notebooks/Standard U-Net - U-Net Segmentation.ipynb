{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"CAMAFfnpeUBv","outputId":"4b69e15d-3207-47da-c02c-451963086821","collapsed":true,"executionInfo":{"status":"ok","timestamp":1747285584109,"user_tz":300,"elapsed":233549,"user":{"displayName":"mk chung","userId":"05412849786518098631"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch\n","  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n","Collecting torchvision\n","  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n","Collecting filelock (from torch)\n","  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting typing-extensions>=4.10.0 (from torch)\n","  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n","Collecting sympy>=1.13.3 (from torch)\n","  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n","Collecting networkx (from torch)\n","  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n","Collecting jinja2 (from torch)\n","  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n","Collecting fsspec (from torch)\n","  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n","  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n","  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n","  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n","  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n","  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n","  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.3.0 (from torch)\n","  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n","Collecting setuptools>=40.8.0 (from triton==3.3.0->torch)\n","  Downloading setuptools-80.7.1-py3-none-any.whl.metadata (6.6 kB)\n","Collecting numpy (from torchvision)\n","  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n","  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n","Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n","  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n","Collecting MarkupSafe>=2.0 (from jinja2->torch)\n","  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n","Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n","Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n","Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading setuptools-80.7.1-py3-none-any.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, setuptools, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision\n","  Attempting uninstall: nvidia-cusparselt-cu12\n","    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n","    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n","      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n","  Attempting uninstall: mpmath\n","    Found existing installation: mpmath 1.3.0\n","    Uninstalling mpmath-1.3.0:\n","      Successfully uninstalled mpmath-1.3.0\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.13.2\n","    Uninstalling typing_extensions-4.13.2:\n","      Successfully uninstalled typing_extensions-4.13.2\n","  Attempting uninstall: sympy\n","    Found existing installation: sympy 1.13.1\n","    Uninstalling sympy-1.13.1:\n","      Successfully uninstalled sympy-1.13.1\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 75.2.0\n","    Uninstalling setuptools-75.2.0:\n","      Successfully uninstalled setuptools-75.2.0\n","  Attempting uninstall: pillow\n","    Found existing installation: pillow 11.2.1\n","    Uninstalling pillow-11.2.1:\n","      Successfully uninstalled pillow-11.2.1\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.4.127\n","    Uninstalling nvidia-nvtx-cu12-12.4.127:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.21.5\n","    Uninstalling nvidia-nccl-cu12-2.21.5:\n","      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: networkx\n","    Found existing installation: networkx 3.4.2\n","    Uninstalling networkx-3.4.2:\n","      Successfully uninstalled networkx-3.4.2\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 3.0.2\n","    Uninstalling MarkupSafe-3.0.2:\n","      Successfully uninstalled MarkupSafe-3.0.2\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","  Attempting uninstall: filelock\n","    Found existing installation: filelock 3.18.0\n","    Uninstalling filelock-3.18.0:\n","      Successfully uninstalled filelock-3.18.0\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.2.0\n","    Uninstalling triton-3.2.0:\n","      Successfully uninstalled triton-3.2.0\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: jinja2\n","    Found existing installation: Jinja2 3.1.6\n","    Uninstalling Jinja2-3.1.6:\n","      Successfully uninstalled Jinja2-3.1.6\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.6.0+cu124\n","    Uninstalling torch-2.6.0+cu124:\n","      Successfully uninstalled torch-2.6.0+cu124\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.21.0+cu124\n","    Uninstalling torchvision-0.21.0+cu124:\n","      Successfully uninstalled torchvision-0.21.0+cu124\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n","fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.3.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.5 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 pillow-11.2.1 setuptools-80.7.1 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 triton-3.3.0 typing-extensions-4.13.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","_distutils_hack","pkg_resources"]},"id":"0a011543fc0947b78660a4f0723926e6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting segmentation-models-pytorch\n","  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.31.1)\n","Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.2.5)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.2.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\n","Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.15)\n","Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.7.0)\n","Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.22.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.13.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.5.1.17)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.3)\n","Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.26.2)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.11.1.6)\n","Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.3.0)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch>=1.8->segmentation-models-pytorch) (80.7.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.4.26)\n","Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: segmentation-models-pytorch\n","Successfully installed segmentation-models-pytorch-0.5.0\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.2.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"]}],"source":["# CS444 U-Net Project Setup for Semantic Segmentation\n","# Standard U-Net on Cityscapes dataset\n","\n","# Step 1: Install Required Libraries\n","!pip install torch torchvision --force-reinstall\n","!pip install segmentation-models-pytorch\n","!pip install matplotlib # Ensure matplotlib is installed for visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6ZJKRlgeaD3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747286470891,"user_tz":300,"elapsed":23920,"user":{"displayName":"mk chung","userId":"05412849786518098631"}},"outputId":"2a110136-9930-40d3-e603-dfd3274649bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Step 2: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["ls '/content/drive/MyDrive/U-Net Segmentation Project/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dTNH29BVUCZ","executionInfo":{"status":"ok","timestamp":1747286606389,"user_tz":300,"elapsed":2042,"user":{"displayName":"mk chung","userId":"05412849786518098631"}},"outputId":"9adedeb5-57dc-4827-9c01-5c6920425ac5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" \u001b[0m\u001b[01;34mCityScapes\u001b[0m/                        'ResNet50 - U-Net_Segmentation.ipynb'\n","'Copy of U-Net_Segmentation.ipynb'   U-Net_Segmentation.ipynb\n","'Final Project Report.gdoc'          U-Net_Segmentation_UNet++.ipynb\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZZIn2y9ecdi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747286606567,"user_tz":300,"elapsed":175,"user":{"displayName":"mk chung","userId":"05412849786518098631"}},"outputId":"d8d1a008-4329-4e40-c94c-9210f97c40ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset structure verified: 'leftImg8bit' and 'gtFine' found in /content/drive/MyDrive/U-Net Segmentation Project/CityScapes.\n"]}],"source":["# Step 3: Verify Dataset Structure\n","import os\n","\n","# IMPORTANT: Update this path to your specific Google Drive location for the Cityscapes dataset\n","root_path = '/content/drive/MyDrive/U-Net Segmentation Project/CityScapes'\n","\n","if not (os.path.exists(os.path.join(root_path, 'leftImg8bit')) and os.path.exists(os.path.join(root_path, 'gtFine'))):\n","    raise RuntimeError(f\"Ensure 'leftImg8bit' and 'gtFine' folders are in the root directory: {root_path}. Please verify the path.\")\n","else:\n","    print(f\"Dataset structure verified: 'leftImg8bit' and 'gtFine' found in {root_path}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3630,"status":"ok","timestamp":1747286610200,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"_pz2bfSLefC2","outputId":"92240ad1-2488-4001-9b52-0c527df13560"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.7.0+cu126\n","CUDA available: True\n","CUDA version: 12.6\n","GPU device: Tesla T4\n"]}],"source":["# Step 4: Verify GPU Environment\n","import torch\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"CUDA version: {torch.version.cuda}\")\n","    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n","    device = torch.device(\"cuda\")\n","else:\n","    print(\"No GPU detected. Training will run on CPU, which will be very slow. Ensure GPU runtime is enabled in Colab (Runtime > Change runtime type).\")\n","    device = torch.device(\"cpu\")\n","\n","# Set environment variables for CUDA debugging\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Synchronous CUDA errors\n","os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"    # Device-side assertions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12061,"status":"ok","timestamp":1747286622253,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"qvwy07W_ehly","outputId":"fb284256-690c-4ee3-a6a6-ed58f3dba4af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded train dataset with 2975 samples.\n","Successfully loaded val dataset with 500 samples.\n","Sample image shape: torch.Size([3, 128, 256])\n","Sample target shape: torch.Size([128, 256])\n","Sample target unique values: tensor([  0,   1,   2,   5,   7,   8,   9,  10,  11,  13,  18, 255])\n"]}],"source":["# Step 5: Load Cityscapes Dataset\n","from torchvision.datasets import Cityscapes\n","from torchvision import transforms\n","import numpy as np\n","from PIL import Image # Needed for target resize resampling\n","\n","# Define transforms (resize to manage memory)\n","# Original image size for Cityscapes is 1024x2048. Resizing significantly.\n","img_height, img_width = 128, 256 # Reduced size for faster training and lower memory\n","\n","transform = transforms.Compose([\n","    transforms.Resize((img_height, img_width)),\n","    transforms.ToTensor()\n","])\n","\n","def target_to_tensor(target):\n","    # Resize target segmentation map. PIL Image object is expected by Resize.\n","    target = target.resize((img_width, img_height), resample=Image.NEAREST) # Nearest neighbor for labels\n","    target_np = np.array(target, dtype=np.uint8)\n","\n","    # Map Cityscapes labels to 0-18 (for 19 classes), set others to ignore_index (255)\n","    label_map = {\n","        # name: id, trainId\n","        'unlabeled': (0, 255), 'ego vehicle': (1, 255), 'rectification border': (2, 255),\n","        'out of roi': (3, 255), 'static': (4, 255), 'dynamic': (5, 255),\n","        'ground': (6, 255), 'road': (7, 0), 'sidewalk': (8, 1),\n","        'parking': (9, 255), 'rail track': (10, 255), 'building': (11, 2),\n","        'wall': (12, 3), 'fence': (13, 4), 'guard rail': (14, 255),\n","        'bridge': (15, 255), 'tunnel': (16, 255), 'pole': (17, 5),\n","        'polegroup': (18, 255), 'traffic light': (19, 6), 'traffic sign': (20, 7),\n","        'vegetation': (21, 8), 'terrain': (22, 9), 'sky': (23, 10),\n","        'person': (24, 11), 'rider': (25, 12), 'car': (26, 13),\n","        'truck': (27, 14), 'bus': (28, 15), 'caravan': (29, 255),\n","        'trailer': (30, 255), 'train': (31, 16), 'motorcycle': (32, 17),\n","        'bicycle': (33, 18)\n","    }\n","    # Create an empty array with ignore_index\n","    mapped_target = np.full_like(target_np, 255, dtype=np.uint8) # 255 for ignore_index\n","\n","    for cityscapes_id_tuple, train_id in label_map.items():\n","        original_id = train_id[0] # Using the 'id' from the tuple for mapping\n","        target_train_id = train_id[1]\n","        if target_train_id != 255: # If it's a valid trainId\n","            mapped_target[target_np == original_id] = target_train_id\n","\n","    return torch.from_numpy(mapped_target).long()\n","\n","# Load training and validation datasets\n","try:\n","    train_dataset = Cityscapes(\n","        root=root_path,\n","        split='train',\n","        mode='fine',\n","        target_type='semantic',\n","        transform=transform,\n","        target_transform=target_to_tensor\n","    )\n","    val_dataset = Cityscapes(\n","        root=root_path,\n","        split='val',\n","        mode='fine',\n","        target_type='semantic',\n","        transform=transform,\n","        target_transform=target_to_tensor\n","    )\n","    print(f\"Successfully loaded train dataset with {len(train_dataset)} samples.\")\n","    print(f\"Successfully loaded val dataset with {len(val_dataset)} samples.\")\n","\n","except Exception as e:\n","    print(f\"Error loading dataset: {e}\")\n","    print(\"Please ensure your `root_path` is correct and the dataset is properly structured.\")\n","    raise\n","\n","# Verify dataset labels from a sample\n","if len(train_dataset) > 0:\n","    sample_image, sample_target = train_dataset[0]\n","    print(f\"Sample image shape: {sample_image.shape}\")\n","    print(f\"Sample target shape: {sample_target.shape}\")\n","    unique_labels = torch.unique(sample_target)\n","    print(f\"Sample target unique values: {unique_labels}\")\n","    if not (all( (unique_labels >= 0) & (unique_labels <= 18) | (unique_labels == 255) )):\n","        print(\"Warning: Unexpected label values found in sample target.\")\n","else:\n","    print(\"Train dataset is empty. Cannot verify sample.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14266,"status":"ok","timestamp":1747286636522,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"wcmRHoRUellz","outputId":"94af9910-a94e-4ac4-ec9c-5d5a1cb8c2b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking train dataset integrity (first 5 samples)...\n","train dataset integrity verified for first 5 samples.\n","Checking val dataset integrity (first 5 samples)...\n","val dataset integrity verified for first 5 samples.\n"]}],"source":["# Step 5.1: Verify Dataset Integrity\n","def check_dataset_integrity(dataset, split_name, num_samples_to_check=5):\n","    print(f\"Checking {split_name} dataset integrity (first {num_samples_to_check} samples)...\")\n","    if len(dataset) == 0:\n","        print(f\"{split_name} dataset is empty. Skipping integrity check.\")\n","        return True\n","    for i in range(min(num_samples_to_check, len(dataset))):\n","        try:\n","            image, target = dataset[i]\n","            # Basic checks\n","            if not isinstance(image, torch.Tensor) or not isinstance(target, torch.Tensor):\n","                print(f\"Error at index {i}: Image or target is not a tensor.\")\n","                return False\n","            if image.shape != torch.Size([3, img_height, img_width]):\n","                 print(f\"Error at index {i}: Unexpected image shape {image.shape}.\")\n","                 return False\n","            if target.shape != torch.Size([img_height, img_width]):\n","                 print(f\"Error at index {i}: Unexpected target shape {target.shape}.\")\n","                 return False\n","            if not ((target >= 0) & (target <= 18) | (target == 255)).all():\n","                 print(f\"Error at index {i}: Target contains invalid labels {torch.unique(target)}.\")\n","                 return False\n","\n","        except Exception as e:\n","            print(f\"Error accessing sample {i} in {split_name} dataset: {e}\")\n","            return False\n","    print(f\"{split_name} dataset integrity verified for first {num_samples_to_check} samples.\")\n","    return True\n","\n","# Check train and validation datasets (first few samples)\n","if not check_dataset_integrity(train_dataset, \"train\"):\n","    raise RuntimeError(\"Train dataset integrity check failed.\")\n","if not check_dataset_integrity(val_dataset, \"val\"):\n","    raise RuntimeError(\"Validation dataset integrity check failed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1747286636528,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"4wQ7hLvpeogt","outputId":"fdcacf8f-d78e-4fa7-d67c-c0e7759d5d8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train loader created with batch size 2.\n","Validation loader created with batch size 2.\n"]}],"source":["# Step 6: Create Data Loaders\n","from torch.utils.data import DataLoader\n","\n","batch_size = 2\n","\n","if len(train_dataset) > 0:\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","else:\n","    train_loader = None\n","    print(\"Train dataset is empty. Train loader not created.\")\n","\n","if len(val_dataset) > 0:\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","else:\n","    val_loader = None\n","    print(\"Validation dataset is empty. Validation loader not created.\")\n","\n","if train_loader:\n","    print(f\"Train loader created with batch size {batch_size}.\")\n","if val_loader:\n","    print(f\"Validation loader created with batch size {batch_size}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237,"referenced_widgets":["9cdbbbfb5b2f424d962f0b68235473bc","d976b329fe7c44e98bfea339a985034c","a3b986948d2a406aa2d7b21d3e58498d","5b906eb13cf44b5aa582862aa6676051","0d6025eb5b4441bb9794efd854b8ebcd","033a5d9bf58c48dfa10951c40c3b998a","2a75247fefb94f07b6c4f34a1e98cf73","649ade3a19854d82b40186f5960318f1","e8be26c082714684bded511cb373ca39","6516b7869c0340848affbf6334ecd7ac","f8bcc51cf69841aaa9f0f4a110ba3ff4","2127fc19dfd34dd7acc78596809f0c2a","75c184c605354eb2a280d0a0d069a567","ea7272f46a734d4e965a04185cdd37a5","369517d323594ab8ad037f58b2d8c236","1d8dc967bc0e4ec7ad4ed551975f6498","5e6719cea2ef4047ac565b4b981a0dac","2e78761a98f44c65934441d7ad6cf218","e7ad722ce65b49178b6ed96f8e4e4d65","6d89fc620a7c44d8a9e2b13df4fe7483","d7e04d436141459c85e540e2dee3eedd","9302efffb97a4a18956b261906882b13"]},"executionInfo":{"elapsed":6571,"status":"ok","timestamp":1747276800618,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"E07uINSEerSA","outputId":"6b3fb4fa-3874-462c-9bdc-faf7e77a8427"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cdbbbfb5b2f424d962f0b68235473bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2127fc19dfd34dd7acc78596809f0c2a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Standard U-Net with resnet34 encoder defined.\n","Attention U-Net (SCSE) with resnet34 encoder defined.\n","U-Net++ with resnet34 encoder defined.\n"]}],"source":["# Step 7: Define Standard U-Net\n","import segmentation_models_pytorch as smp\n","\n","num_classes = 19  # Based on the label mapping (0-18)\n","encoder_name = \"resnet34\"\n","\n","# Standard U-Net\n","model_standard = smp.Unet(\n","    encoder_name=encoder_name,\n","    encoder_weights=\"imagenet\", # Using pre-trained weights can help convergence\n","    in_channels=3,\n","    classes=num_classes,\n","    activation=None # Raw logits for CrossEntropyLoss\n",")\n","print(f\"Standard U-Net with {encoder_name} encoder defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1747276800632,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"UsTMZjp8eubU","outputId":"d0a6842d-409e-4222-91c6-df7283c6b893"},"outputs":[{"output_type":"stream","name":"stdout","text":["CrossEntropyLoss defined with ignore_index=255.\n"]}],"source":["# Step 8: Define Loss Function\n","import torch.nn as nn\n","# CrossEntropyLoss expects raw logits from the model and long type targets.\n","# ignore_index=255 means that pixels with label 255 will not contribute to the loss.\n","criterion = nn.CrossEntropyLoss(ignore_index=255)\n","print(\"CrossEntropyLoss defined with ignore_index=255.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1747276800645,"user":{"displayName":"mk chung","userId":"05412849786518098631"},"user_tz":300},"id":"tn_l-Gsvg6El","outputId":"0886e087-5477-4661-e859-1dde30c850bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Models will be saved in: /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models\n"]}],"source":["# Step 8.1: Setup Model Saving and Validation Loss Function\n","import os\n","\n","save_dir = os.path.join(root_path, 'saved_models')\n","os.makedirs(save_dir, exist_ok=True)\n","print(f\"Models will be saved in: {save_dir}\")\n","\n","def evaluate_validation_loss(model, loader, criterion, device, model_name):\n","    model.eval() # Set model to evaluation mode\n","    running_val_loss = 0.0\n","    num_batches = len(loader)\n","\n","    if num_batches == 0:\n","        print(f\"Warning: Validation loader for {model_name} is empty. Cannot compute validation loss.\")\n","        return float('inf') # Return infinity if no validation can be done\n","\n","    with torch.no_grad(): # No gradients needed for validation\n","        for images, targets in loader:\n","            images, targets = images.to(device), targets.to(device)\n","\n","            # Ensure targets are valid before calculating loss\n","            if not ((targets >= 0) & (targets < num_classes) | (targets == 255)).all():\n","                print(f\"Validation: Invalid labels detected in targets for {model_name}! Unique: {torch.unique(targets)}. Skipping batch for loss calculation.\")\n","                pass\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, targets)\n","            running_val_loss += loss.item()\n","\n","    avg_val_loss = running_val_loss / num_batches if num_batches > 0 else float('inf')\n","    if device.type == \"cuda\":\n","        torch.cuda.empty_cache()\n","    return avg_val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsHCbdHPexJ-"},"outputs":[],"source":["# Step 9: Training Configuration\n","num_epochs = 15 # Increased number of epochs\n","learning_rate = 0.001\n","weight_decay = 1e-5 # Added weight decay for regularization\n","\n","# Helper function for training one epoch\n","def train_one_epoch(model, loader, optimizer, criterion, device, model_name):\n","    model.train()\n","    running_loss = 0.0\n","    num_batches = len(loader)\n","\n","    if num_batches == 0:\n","        print(f\"Warning: Training loader for {model_name} is empty. Skipping training epoch.\")\n","        return 0.0\n","\n","    for i, (images, targets) in enumerate(loader):\n","        images, targets = images.to(device), targets.to(device)\n","\n","        try:\n","            # Verify target labels before passing to model/loss\n","            if not ((targets >= 0) & (targets < num_classes) | (targets == 255)).all():\n","                print(f\"Batch {i}: Invalid labels detected in targets for {model_name}! Unique: {torch.unique(targets)}. Skipping batch.\")\n","                problematic_targets = targets.clone()\n","                problematic_targets[(targets >= 0) & (targets < num_classes) | (targets == 255)] = -1 # Mark valid ones\n","                print(f\"Problematic target values: {torch.unique(problematic_targets.masked_select(problematic_targets != -1))}\")\n","                continue # Skip this batch\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","            if (i + 1) % (num_batches // 5 if num_batches >= 5 else 1) == 0: # Print 5 times per epoch\n","                print(f\"    {model_name} - Batch {i+1}/{num_batches}, Loss: {loss.item():.4f}\")\n","\n","        except RuntimeError as e:\n","            print(f\"RuntimeError during training {model_name} at batch {i}: {e}\")\n","            if \"CUDA out of memory\" in str(e):\n","                print(\"CUDA OOM: Try reducing batch size or image dimensions.\")\n","                if device.type == \"cuda\": torch.cuda.empty_cache()\n","            return float('inf') # Indicate critical error\n","\n","    epoch_loss = running_loss / num_batches if num_batches > 0 else 0\n","    if device.type == \"cuda\":\n","        torch.cuda.empty_cache()\n","    return epoch_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zSEpggTPezv7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747259687979,"user_tz":300,"elapsed":7276666,"user":{"displayName":"mk chung","userId":"05412849786518098631"}},"outputId":"19e5f365-3573-4637-dbdc-bd6357068634"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Training Standard U-Net (resnet34) ---\n","Initial GPU memory allocated for Standard U-Net: 383.80 MB\n","Epoch 1/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.6235\n","    Standard U-Net - Batch 594/1488, Loss: 0.6721\n","    Standard U-Net - Batch 891/1488, Loss: 1.0864\n","    Standard U-Net - Batch 1188/1488, Loss: 0.5789\n","    Standard U-Net - Batch 1485/1488, Loss: 0.5510\n","  Standard U-Net - Epoch 1 Average Training Loss: 0.7403\n","  Standard U-Net - Epoch 1 Average Validation Loss: 0.6833\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_best_val.pth (Val Loss: 0.6833)\n","Epoch 2/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.5768\n","    Standard U-Net - Batch 594/1488, Loss: 0.4980\n","    Standard U-Net - Batch 891/1488, Loss: 0.4547\n","    Standard U-Net - Batch 1188/1488, Loss: 0.5135\n","    Standard U-Net - Batch 1485/1488, Loss: 0.3480\n","  Standard U-Net - Epoch 2 Average Training Loss: 0.6174\n","  Standard U-Net - Epoch 2 Average Validation Loss: 0.6049\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_best_val.pth (Val Loss: 0.6049)\n","Epoch 3/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.3590\n","    Standard U-Net - Batch 594/1488, Loss: 1.1718\n","    Standard U-Net - Batch 891/1488, Loss: 0.5880\n","    Standard U-Net - Batch 1188/1488, Loss: 0.4985\n","    Standard U-Net - Batch 1485/1488, Loss: 0.9846\n","  Standard U-Net - Epoch 3 Average Training Loss: 0.5513\n","  Standard U-Net - Epoch 3 Average Validation Loss: 0.5771\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_best_val.pth (Val Loss: 0.5771)\n","Epoch 4/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.3552\n","    Standard U-Net - Batch 594/1488, Loss: 0.5400\n","    Standard U-Net - Batch 891/1488, Loss: 0.5462\n","    Standard U-Net - Batch 1188/1488, Loss: 0.3299\n","    Standard U-Net - Batch 1485/1488, Loss: 0.6026\n","  Standard U-Net - Epoch 4 Average Training Loss: 0.5132\n","  Standard U-Net - Epoch 4 Average Validation Loss: 0.7523\n","Epoch 5/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.5159\n","    Standard U-Net - Batch 594/1488, Loss: 0.5094\n","    Standard U-Net - Batch 891/1488, Loss: 0.4855\n","    Standard U-Net - Batch 1188/1488, Loss: 0.4387\n","    Standard U-Net - Batch 1485/1488, Loss: 0.4647\n","  Standard U-Net - Epoch 5 Average Training Loss: 0.4869\n","  Standard U-Net - Epoch 5 Average Validation Loss: 0.5986\n","Epoch 6/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.3952\n","    Standard U-Net - Batch 594/1488, Loss: 0.6515\n","    Standard U-Net - Batch 891/1488, Loss: 0.2810\n","    Standard U-Net - Batch 1188/1488, Loss: 0.6634\n","    Standard U-Net - Batch 1485/1488, Loss: 0.3893\n","  Standard U-Net - Epoch 6 Average Training Loss: 0.4613\n","  Standard U-Net - Epoch 6 Average Validation Loss: 0.4849\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_best_val.pth (Val Loss: 0.4849)\n","Epoch 7/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.5203\n","    Standard U-Net - Batch 594/1488, Loss: 0.4484\n","    Standard U-Net - Batch 891/1488, Loss: 0.2918\n","    Standard U-Net - Batch 1188/1488, Loss: 0.4262\n","    Standard U-Net - Batch 1485/1488, Loss: 0.4115\n","  Standard U-Net - Epoch 7 Average Training Loss: 0.4439\n","  Standard U-Net - Epoch 7 Average Validation Loss: 0.4648\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_best_val.pth (Val Loss: 0.4648)\n","Epoch 8/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.1826\n","    Standard U-Net - Batch 594/1488, Loss: 0.2101\n","    Standard U-Net - Batch 891/1488, Loss: 0.3820\n","    Standard U-Net - Batch 1188/1488, Loss: 0.5266\n","    Standard U-Net - Batch 1485/1488, Loss: 0.2934\n","  Standard U-Net - Epoch 8 Average Training Loss: 0.4240\n","  Standard U-Net - Epoch 8 Average Validation Loss: 0.5609\n","Epoch 9/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.4160\n","    Standard U-Net - Batch 594/1488, Loss: 0.4295\n","    Standard U-Net - Batch 891/1488, Loss: 0.2727\n","    Standard U-Net - Batch 1188/1488, Loss: 0.3325\n","    Standard U-Net - Batch 1485/1488, Loss: 0.4135\n","  Standard U-Net - Epoch 9 Average Training Loss: 0.4102\n","  Standard U-Net - Epoch 9 Average Validation Loss: 0.4941\n","Epoch 10/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.4037\n","    Standard U-Net - Batch 594/1488, Loss: 0.4293\n","    Standard U-Net - Batch 891/1488, Loss: 0.5412\n","    Standard U-Net - Batch 1188/1488, Loss: 0.8260\n","    Standard U-Net - Batch 1485/1488, Loss: 0.2493\n","  Standard U-Net - Epoch 10 Average Training Loss: 0.3948\n","  Standard U-Net - Epoch 10 Average Validation Loss: 0.4719\n","Epoch 11/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.3129\n","    Standard U-Net - Batch 594/1488, Loss: 0.2957\n","    Standard U-Net - Batch 891/1488, Loss: 0.4057\n","    Standard U-Net - Batch 1188/1488, Loss: 0.3659\n","    Standard U-Net - Batch 1485/1488, Loss: 0.3623\n","  Standard U-Net - Epoch 11 Average Training Loss: 0.3848\n","  Standard U-Net - Epoch 11 Average Validation Loss: 0.4515\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_best_val.pth (Val Loss: 0.4515)\n","Epoch 12/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.3035\n","    Standard U-Net - Batch 594/1488, Loss: 0.3514\n","    Standard U-Net - Batch 891/1488, Loss: 0.4301\n","    Standard U-Net - Batch 1188/1488, Loss: 0.2624\n","    Standard U-Net - Batch 1485/1488, Loss: 0.3991\n","  Standard U-Net - Epoch 12 Average Training Loss: 0.3714\n","  Standard U-Net - Epoch 12 Average Validation Loss: 0.4948\n","Epoch 13/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.4680\n","    Standard U-Net - Batch 594/1488, Loss: 0.4016\n","    Standard U-Net - Batch 891/1488, Loss: 0.4149\n","    Standard U-Net - Batch 1188/1488, Loss: 0.2115\n","    Standard U-Net - Batch 1485/1488, Loss: 0.3222\n","  Standard U-Net - Epoch 13 Average Training Loss: 0.3634\n","  Standard U-Net - Epoch 13 Average Validation Loss: 0.4303\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_best_val.pth (Val Loss: 0.4303)\n","Epoch 14/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.3745\n","    Standard U-Net - Batch 594/1488, Loss: 0.2980\n","    Standard U-Net - Batch 891/1488, Loss: 0.4338\n","    Standard U-Net - Batch 1188/1488, Loss: 0.3710\n","    Standard U-Net - Batch 1485/1488, Loss: 0.4318\n","  Standard U-Net - Epoch 14 Average Training Loss: 0.3510\n","  Standard U-Net - Epoch 14 Average Validation Loss: 0.4557\n","Epoch 15/15\n","    Standard U-Net - Batch 297/1488, Loss: 0.2843\n","    Standard U-Net - Batch 594/1488, Loss: 0.2878\n","    Standard U-Net - Batch 891/1488, Loss: 0.4302\n","    Standard U-Net - Batch 1188/1488, Loss: 0.3102\n","    Standard U-Net - Batch 1485/1488, Loss: 0.3295\n","  Standard U-Net - Epoch 15 Average Training Loss: 0.3434\n","  Standard U-Net - Epoch 15 Average Validation Loss: 0.4299\n","    New best model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_best_val.pth (Val Loss: 0.4299)\n","--- Standard U-Net Training Finished. Final model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_final_epoch.pth ---\n","Best validation model saved at /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_best_val.pth with val_loss: 0.4299\n"]}],"source":["# --- Training Cell 9.1: Standard U-Net ---\n","print(f\"\\n--- Training Standard U-Net ({encoder_name}) ---\")\n","model_name_standard = \"standard_unet\"\n","best_model_standard_path = os.path.join(save_dir, f\"{model_name_standard}_best_val.pth\")\n","final_model_standard_path = os.path.join(save_dir, f\"{model_name_standard}_final_epoch.pth\")\n","best_val_loss_standard = float('inf')\n","\n","if train_loader:\n","    model_standard.to(device)\n","    optimizer_standard = torch.optim.Adam(model_standard.parameters(), lr=learning_rate)\n","\n","    if device.type == \"cuda\":\n","        print(f\"Initial GPU memory allocated for Standard U-Net: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        # Training phase\n","        epoch_loss_standard = train_one_epoch(model_standard, train_loader, optimizer_standard, criterion, device, \"Standard U-Net\")\n","        if epoch_loss_standard == float('inf'):\n","            print(f\"Critical error training Standard U-Net in epoch {epoch+1}. Stopping training for this model.\")\n","            break\n","        print(f\"  Standard U-Net - Epoch {epoch+1} Average Training Loss: {epoch_loss_standard:.4f}\")\n","\n","        # Validation phase\n","        if val_loader:\n","            current_val_loss = evaluate_validation_loss(model_standard, val_loader, criterion, device, \"Standard U-Net (Validation)\")\n","            print(f\"  Standard U-Net - Epoch {epoch+1} Average Validation Loss: {current_val_loss:.4f}\")\n","\n","            if current_val_loss < best_val_loss_standard:\n","                best_val_loss_standard = current_val_loss\n","                torch.save(model_standard.state_dict(), best_model_standard_path)\n","                print(f\"    New best model saved to {best_model_standard_path} (Val Loss: {current_val_loss:.4f})\")\n","        else:\n","            print(\"  Skipping validation for checkpointing as val_loader is not available.\")\n","\n","    # Save the final model state after all epochs\n","    torch.save(model_standard.state_dict(), final_model_standard_path)\n","    print(f\"--- Standard U-Net Training Finished. Final model saved to {final_model_standard_path} ---\")\n","    if val_loader and os.path.exists(best_model_standard_path):\n","        print(f\"Best validation model saved at {best_model_standard_path} with val_loss: {best_val_loss_standard:.4f}\")\n","else:\n","    print(\"Skipping Standard U-Net training as train_loader is not available.\")"]},{"cell_type":"code","source":["# --- Training Cell 9.1.1: Fine-tune Standard U-Net ---\n","print(f\"\\n--- Fine-tuning Standard U-Net ({encoder_name}) ---\")\n","\n","# Define parameters for fine-tuning\n","num_ft_epochs = 5\n","fine_tune_lr = 5e-5\n","\n","# Fine-tuned model save paths\n","model_name_standard_ft = \"standard_unet_ft\"\n","best_model_standard_ft_path = os.path.join(save_dir, f\"{model_name_standard_ft}_best_val.pth\")\n","final_model_standard_ft_path = os.path.join(save_dir, f\"{model_name_standard_ft}_final_epoch.pth\")\n","\n","# Path to the pretrained best model\n","pretrained_model_standard_path = os.path.join(save_dir, \"standard_unet_best_val.pth\")\n","\n","# Best val loss init (you can also use float('inf') if unknown)\n","best_val_loss_standard_ft = best_val_loss_standard if 'best_val_loss_standard' in globals() else float('inf')\n","\n","if not os.path.exists(pretrained_model_standard_path):\n","    print(f\"Error: Pretrained model not found at {pretrained_model_standard_path}. Cannot fine-tune.\")\n","else:\n","    if train_loader and val_loader:\n","        if 'model_standard' not in globals() or model_standard is None:\n","            print(\"Re-defining Standard U-Net model architecture for fine-tuning...\")\n","            model_standard = smp.Unet(\n","                encoder_name=encoder_name,\n","                encoder_weights=None,\n","                in_channels=3,\n","                classes=num_classes,\n","                activation=None\n","            )\n","            print(\"Standard U-Net model re-defined.\")\n","        else:\n","            print(\"Using existing 'model_standard' instance.\")\n","\n","        # Load the best model weights\n","        print(f\"Loading weights from {pretrained_model_standard_path} for fine-tuning...\")\n","        try:\n","            model_standard.load_state_dict(torch.load(pretrained_model_standard_path, map_location=device))\n","            print(\"Successfully loaded pre-trained weights.\")\n","        except Exception as e:\n","            print(f\"Error loading pre-trained weights: {e}\")\n","            raise\n","\n","        model_standard.to(device)\n","\n","        # Fine-tuning optimizer and scheduler\n","        optimizer_standard_ft = torch.optim.Adam(model_standard.parameters(), lr=fine_tune_lr, weight_decay=weight_decay)\n","        scheduler_standard_ft = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_standard_ft, mode='min', factor=0.1, patience=3)\n","\n","        if device.type == \"cuda\":\n","            torch.cuda.empty_cache()\n","            print(f\"Initial GPU memory allocated for Fine-tuning Standard U-Net: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n","\n","        print(f\"Starting fine-tuning for {num_ft_epochs} epochs with LR: {fine_tune_lr}\")\n","        for epoch in range(num_ft_epochs):\n","            current_epoch_ft = epoch + 1\n","            print(f\"Fine-tuning Epoch {current_epoch_ft}/{num_ft_epochs}. Current LR: {optimizer_standard_ft.param_groups[0]['lr']:.2e}\")\n","\n","            # Train phase\n","            epoch_loss_standard_ft = train_one_epoch(model_standard, train_loader, optimizer_standard_ft, criterion, device, \"Standard U-Net (Fine-tuning)\")\n","            if epoch_loss_standard_ft == float('inf'):\n","                print(f\"Critical error fine-tuning Standard U-Net in epoch {current_epoch_ft}. Stopping.\")\n","                break\n","            print(f\"  Standard U-Net (Fine-tuning) - Epoch {current_epoch_ft} Avg Training Loss: {epoch_loss_standard_ft:.4f}\")\n","\n","            # Validation phase\n","            current_val_loss_ft = evaluate_validation_loss(model_standard, val_loader, criterion, device, \"Standard U-Net (Fine-tuning Validation)\")\n","            print(f\"  Standard U-Net (Fine-tuning) - Epoch {current_epoch_ft} Avg Validation Loss: {current_val_loss_ft:.4f}\")\n","\n","            if current_val_loss_ft < best_val_loss_standard_ft:\n","                best_val_loss_standard_ft = current_val_loss_ft\n","                torch.save(model_standard.state_dict(), best_model_standard_ft_path)\n","                print(f\"    New best fine-tuned model saved to {best_model_standard_ft_path} (Val Loss: {current_val_loss_ft:.4f})\")\n","\n","            # Scheduler step\n","            scheduler_standard_ft.step(current_val_loss_ft)\n","\n","        # Save final model\n","        torch.save(model_standard.state_dict(), final_model_standard_ft_path)\n","        print(f\"--- Standard U-Net Fine-tuning Finished. Final model saved to {final_model_standard_ft_path} ---\")\n","        if os.path.exists(best_model_standard_ft_path):\n","            print(f\"Best fine-tuned validation model saved at {best_model_standard_ft_path} with val_loss: {best_val_loss_standard_ft:.4f}\")\n","    else:\n","        if not train_loader: print(\"Skipping fine-tuning as train_loader is not available.\")\n","        if not val_loader: print(\"Skipping fine-tuning as val_loader is not available.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NQlNhmTA1LSY","executionInfo":{"status":"ok","timestamp":1747262311533,"user_tz":300,"elapsed":1897325,"user":{"displayName":"mk chung","userId":"05412849786518098631"}},"outputId":"6357a0f9-fdb9-44b1-9464-2209e890b99d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Fine-tuning Standard U-Net (resnet34) ---\n","Using existing 'model_standard' instance.\n","Loading weights from /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_best_val.pth for fine-tuning...\n","Successfully loaded pre-trained weights.\n","Initial GPU memory allocated for Fine-tuning Standard U-Net: 574.89 MB\n","Starting fine-tuning for 5 epochs with LR: 5e-05\n","Fine-tuning Epoch 1/5. Current LR: 5.00e-05\n","    Standard U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.2027\n","    Standard U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.3296\n","    Standard U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.3963\n","    Standard U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.3828\n","    Standard U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.3900\n","  Standard U-Net (Fine-tuning) - Epoch 1 Avg Training Loss: 0.3069\n","  Standard U-Net (Fine-tuning) - Epoch 1 Avg Validation Loss: 0.3948\n","    New best fine-tuned model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_ft_best_val.pth (Val Loss: 0.3948)\n","Fine-tuning Epoch 2/5. Current LR: 5.00e-05\n","    Standard U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.1668\n","    Standard U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.3076\n","    Standard U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.3389\n","    Standard U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.4831\n","    Standard U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.2223\n","  Standard U-Net (Fine-tuning) - Epoch 2 Avg Training Loss: 0.2979\n","  Standard U-Net (Fine-tuning) - Epoch 2 Avg Validation Loss: 0.3985\n","Fine-tuning Epoch 3/5. Current LR: 5.00e-05\n","    Standard U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.2189\n","    Standard U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.3708\n","    Standard U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.3056\n","    Standard U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.3515\n","    Standard U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.3180\n","  Standard U-Net (Fine-tuning) - Epoch 3 Avg Training Loss: 0.2941\n","  Standard U-Net (Fine-tuning) - Epoch 3 Avg Validation Loss: 0.3943\n","    New best fine-tuned model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_ft_best_val.pth (Val Loss: 0.3943)\n","Fine-tuning Epoch 4/5. Current LR: 5.00e-05\n","    Standard U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.2155\n","    Standard U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.3812\n","    Standard U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.3137\n","    Standard U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.2628\n","    Standard U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.2897\n","  Standard U-Net (Fine-tuning) - Epoch 4 Avg Training Loss: 0.2909\n","  Standard U-Net (Fine-tuning) - Epoch 4 Avg Validation Loss: 0.3898\n","    New best fine-tuned model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_ft_best_val.pth (Val Loss: 0.3898)\n","Fine-tuning Epoch 5/5. Current LR: 5.00e-05\n","    Standard U-Net (Fine-tuning) - Batch 297/1488, Loss: 0.3088\n","    Standard U-Net (Fine-tuning) - Batch 594/1488, Loss: 0.3284\n","    Standard U-Net (Fine-tuning) - Batch 891/1488, Loss: 0.2729\n","    Standard U-Net (Fine-tuning) - Batch 1188/1488, Loss: 0.1151\n","    Standard U-Net (Fine-tuning) - Batch 1485/1488, Loss: 0.2578\n","  Standard U-Net (Fine-tuning) - Epoch 5 Avg Training Loss: 0.2889\n","  Standard U-Net (Fine-tuning) - Epoch 5 Avg Validation Loss: 0.3888\n","    New best fine-tuned model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_ft_best_val.pth (Val Loss: 0.3888)\n","--- Standard U-Net Fine-tuning Finished. Final model saved to /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_ft_final_epoch.pth ---\n","Best fine-tuned validation model saved at /content/drive/MyDrive/U-Net Segmentation Project/CityScapes/saved_models/standard_unet_ft_best_val.pth with val_loss: 0.3888\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1_aQzJ_fR-AcQcVYYLwxgZ5gvtN4h2M6a","timestamp":1747251036282}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9cdbbbfb5b2f424d962f0b68235473bc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d976b329fe7c44e98bfea339a985034c","IPY_MODEL_a3b986948d2a406aa2d7b21d3e58498d","IPY_MODEL_5b906eb13cf44b5aa582862aa6676051"],"layout":"IPY_MODEL_0d6025eb5b4441bb9794efd854b8ebcd"}},"d976b329fe7c44e98bfea339a985034c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_033a5d9bf58c48dfa10951c40c3b998a","placeholder":"​","style":"IPY_MODEL_2a75247fefb94f07b6c4f34a1e98cf73","value":"config.json: 100%"}},"a3b986948d2a406aa2d7b21d3e58498d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_649ade3a19854d82b40186f5960318f1","max":156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e8be26c082714684bded511cb373ca39","value":156}},"5b906eb13cf44b5aa582862aa6676051":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6516b7869c0340848affbf6334ecd7ac","placeholder":"​","style":"IPY_MODEL_f8bcc51cf69841aaa9f0f4a110ba3ff4","value":" 156/156 [00:00&lt;00:00, 11.0kB/s]"}},"0d6025eb5b4441bb9794efd854b8ebcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"033a5d9bf58c48dfa10951c40c3b998a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a75247fefb94f07b6c4f34a1e98cf73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"649ade3a19854d82b40186f5960318f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8be26c082714684bded511cb373ca39":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6516b7869c0340848affbf6334ecd7ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8bcc51cf69841aaa9f0f4a110ba3ff4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2127fc19dfd34dd7acc78596809f0c2a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75c184c605354eb2a280d0a0d069a567","IPY_MODEL_ea7272f46a734d4e965a04185cdd37a5","IPY_MODEL_369517d323594ab8ad037f58b2d8c236"],"layout":"IPY_MODEL_1d8dc967bc0e4ec7ad4ed551975f6498"}},"75c184c605354eb2a280d0a0d069a567":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e6719cea2ef4047ac565b4b981a0dac","placeholder":"​","style":"IPY_MODEL_2e78761a98f44c65934441d7ad6cf218","value":"model.safetensors: 100%"}},"ea7272f46a734d4e965a04185cdd37a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7ad722ce65b49178b6ed96f8e4e4d65","max":87275112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d89fc620a7c44d8a9e2b13df4fe7483","value":87275112}},"369517d323594ab8ad037f58b2d8c236":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7e04d436141459c85e540e2dee3eedd","placeholder":"​","style":"IPY_MODEL_9302efffb97a4a18956b261906882b13","value":" 87.3M/87.3M [00:00&lt;00:00, 358MB/s]"}},"1d8dc967bc0e4ec7ad4ed551975f6498":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e6719cea2ef4047ac565b4b981a0dac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e78761a98f44c65934441d7ad6cf218":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7ad722ce65b49178b6ed96f8e4e4d65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d89fc620a7c44d8a9e2b13df4fe7483":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7e04d436141459c85e540e2dee3eedd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9302efffb97a4a18956b261906882b13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}